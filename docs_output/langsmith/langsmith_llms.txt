# Document Index
- [Get started with LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/.md): LangSmith is a platform for building LLM applications, offering features for observability, evals, and prompt engineering. It provides tools for analyzing traces, evaluating application performance with human feedback, and iterating on prompts with version control. LangSmith supports integration with LangChain and LangGraph frameworks.
- [LangSmith Get Started Guide](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/.md): LangSmith is a platform for building LLM applications, offering features for observability, evals, and prompt engineering. It integrates with LangChain and LangGraph, providing tools to monitor, evaluate, and iterate on prompts for production-grade AI applications.
- [Prompt Engineering How-To Guides](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/prompt_engineering/how_to_guides.md): This document provides a comprehensive guide to prompt engineering using LangSmith, covering prompt organization, management, and iteration within the Playground. It details steps for creating, updating, and managing prompts, utilizing datasets for few-shot prompting, and integrating with various model providers.
- [Get started with LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/.md): LangSmith is a platform for building production-grade LLM applications, offering features for observability, evals, and prompt engineering. It helps monitor and evaluate applications, enabling rapid and confident deployment. The platform integrates with LangChain OSS and provides tools for prompt iteration, performance analysis, and human feedback collection.
- [Get started with LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/.md): LangSmith is a platform for building LLM applications, offering tools for observability, evals, and prompt engineering. It supports integration with LangChain OSS and provides features like trace analysis, performance evaluation, and prompt iteration to help developers build and debug LLM applications effectively.
- [Prompt Engineering How-To Guides](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/prompt_engineering/how_to_guides.md): A collection of step-by-step guides for prompt engineering tasks in LangSmith, covering prompt organization, management, and iteration using the Playground and Datasets for few-shot prompting.
- [Prompt Engineering with LangSmith Playground](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/prompt_engineering/how_to_guides/create_a_prompt.md): This document explains how to use the LangSmith Playground to compose prompts, including managing messages with roles (system, human, AI), setting template formats (f-string, mustache), and adding dynamic content through template variables.
- [Comparing Experiment Results in LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/compare_experiment_results.md): This document provides a guide on how to compare experiment results in LangSmith. It covers opening the comparison view, adjusting table display settings, identifying regressions and improvements, filtering results, updating baseline experiments and metrics, and utilizing charts and metadata for analysis.
- [LangSmith Evaluation How-To Guides](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides.md): A comprehensive guide to LangSmith's evaluation features, covering dataset management, offline and online evaluations, analysis of experiment results, and annotation queues for collecting feedback.
- [Prebuilt Evaluators in LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/prebuilt_evaluators.md): LangSmith integrates with the open-source openevals package to provide a suite of prebuilt evaluators for immediate use. This guide demonstrates setting up and running an LLM-as-a-judge evaluator using Python and TypeScript, including integration with testing frameworks and the LangSmith evaluate method. It covers installing necessary packages, setting API keys, and structuring test files for evaluation.
- [Dashboards in LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/dashboards.md): LangSmith offers prebuilt and custom dashboards to monitor project health and trace data. Prebuilt dashboards provide high-level insights automatically, while custom dashboards allow tailored collections of charts. The page details how to create, configure, and manage custom dashboards, including selecting metrics, filtering runs, and splitting data by grouping or series.
- [Trace with LangChain (Python and JS/TS)](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/trace_with_langchain.md): This document explains how to integrate LangSmith with LangChain for tracing LLM applications. It covers installation, environment configuration, logging traces with Python and JavaScript, selective tracing using callbacks or context managers, and logging to specific projects.
- [Changelog LangSmith App](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/changelog.md): This document details the updates for the LangSmith App, including new features, enhancements, and bug fixes across various releases from April to May 2025. Key updates involve usage tracking, cost monitoring, prompt management, agent observability, and improvements to dataset and evaluation functionalities.
- [LangChain Off-the-Shelf Evaluators](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/sdk_reference/langchain_evaluators.md): LangChain provides readily available evaluators for common evaluation tasks, with support currently limited to Python. These evaluators, while useful, should be integrated into a comprehensive testing strategy rather than blindly trusted, especially for LLM-based metrics which are most reliable in aggregate over larger datasets.
- [Regions FAQ](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/regions_faq.md): This document addresses frequently asked questions about LangSmith's regions, specifically focusing on data hosting, legal compliance (GDPR, SOC 2, HIPAA), feature availability, and plan differences between the US and EU instances. It clarifies that while core features are equivalent, there might be slight launch delays between regions. The FAQ also covers organizational workspace limitations and the inability to switch organizations between regions at this time.
- [Dataset Transformations in LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/evaluation/dataset_transformations.md): LangSmith enables data transformations on dataset schemas for preprocessing, allowing data to align with various formats like OpenAI's standard message structure. It provides specific transformation types such as 'remove_system_messages', 'convert_to_openai_message', 'convert_to_openai_tool', and 'remove_extra_fields' to facilitate this. The platform also offers a prebuilt Chat Model schema to simplify the collection and standardization of data from LLM runs, ensuring compatibility for downstream evaluations and few-shot prompting.
- [Authentication Methods](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/authentication_authorization/authentication_methods.md): LangSmith offers various authentication methods for both cloud and self-hosted users, including email/password, social providers (GitHub, Google), SAML SSO for enterprises, and OAuth 2.0/OIDC for self-hosted instances.
- [Cloud Architecture and Scalability of LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/cloud_architecture_and_scalability.md): This document details the cloud architecture and scalability of LangSmith, a platform for LLM application observability and evaluation. It covers the regional deployment on Google Cloud Platform (GCP), including US and EU services, storage solutions like Supabase and ClickHouse, and various GCP services utilized. The document also provides information on whitelisting IP addresses for enhanced connectivity and security.
- [Self-Hosting LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/self_hosting.md): Step-by-step guides for installing, configuring, and scaling a self-hosted LangSmith instance, covering architectural overviews, deployment options (Kubernetes, Docker), configuration settings (SSO, databases, auth, blob storage, TTLs), usage, organization charts, upgrades, egress requirements, and troubleshooting.
- [Manage Prompts Programmatically with LangSmith SDK](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/prompt_engineering/how_to_guides/manage_prompts_programatically.md): This document provides a guide on how to manage prompts programmatically using the LangSmith Python and TypeScript SDKs. It covers installation, environment configuration, and demonstrates how to push and pull prompts, including those with associated model configurations, using code examples in Python and TypeScript.
- [LangSmith UI Quick Start for Prompt Engineering](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/prompt_engineering/quickstarts/quickstart_ui.md): This guide provides a step-by-step walkthrough on creating, testing, saving, and iterating on prompts using the LangSmith UI. It covers detailed instructions and includes visual aids for each stage of the prompt engineering process.
- [Dynamic Few Shot Example Selection](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection.md): This document explains how to configure datasets in LangSmith for dynamic few-shot example selection. It covers pre-conditions like using the KV store data type and having a defined input schema, and guides users through indexing their dataset for efficient search. The feature is currently in open beta and requires a paid team plan and LangSmith cloud access. It also details how to test search quality in the playground and integrate few-shot search into applications using provided code snippets for various languages.
- [Evaluation Quick Start Guide for LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation.md): This guide explains how to use LangSmith for evaluating LLM applications. Evaluations involve datasets, target functions, and evaluators to quantify performance and identify issues. It covers setting up dependencies, API keys, and environment variables, then demonstrates creating datasets and defining target functions using Python and TypeScript examples. Finally, it shows how to implement evaluators, including pre-built options from openevals.
- [Concepts in LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/concepts.md): This guide explains key LangSmith concepts like Runs, Traces, and Projects. It also covers Feedback, Tags, and Metadata for organizing and analyzing LLM application data, along with details on data storage, retention, and trace deletion policies.
- [How to Print Detailed Logs with LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/output_detailed_logs.md): This document explains how to configure and increase the verbosity of logs when using the LangSmith Python SDK. It details how to ensure logs are sent to standard output and how to set the logging level to DEBUG for more detailed information, which is helpful for debugging.
- [Trace with LangGraph (Python and JS/TS)](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/trace_with_langgraph.md): This document explains how to trace agentic workflows in LangGraph using both Python and JavaScript/TypeScript. It covers setup, environment configuration, and logging traces with LangChain modules and other SDKs, providing code examples for each scenario and illustrating the integration with LangSmith for visualization.
- [langsmith.evaluation._runner.evaluate](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/python/evaluation/langsmith.evaluation._runner.evaluate): This function evaluates a target system on a given dataset, with options for specifying evaluators, summary evaluators, metadata, and experiment configurations. It supports various input types for the target system and dataset, and returns experiment results.
- [Webhook Notifications for Rules](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/webhooks.md): This document explains how to set up webhook notifications for rules in LangSmith. It details the structure of the webhook payload, security best practices like using secret keys, and provides an example of setting up a webhook with Modal, including code snippets for the service and explanations for configuration.
- [Trace LangChain with OpenTelemetry](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/trace_langchain_with_otel.md): This document provides a guide on integrating LangChain applications with OpenTelemetry for tracing. It covers sending traces to LangSmith, exporting to other OTLP-compatible providers, and implementing distributed tracing. The guide includes installation steps, configuration examples using environment variables and code, and demonstrates how to view traces in LangSmith. It also explains how to use the OpenTelemetry Collector for fanning out telemetry data to multiple destinations, which offers advantages like centralized configuration and improved scalability.
- [Troubleshooting Variable Caching in LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/toubleshooting_variable_caching.md): This document provides a guide to troubleshooting variable caching issues in LangSmith, which can occur when traces are not appearing in the expected project or are logged to the wrong workspace. It details steps to verify environment variables, clear the cache, and reload variables from a .env file to resolve these issues, particularly in Jupyter notebook environments.
- [AsyncClient — LangSmith documentation](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/python/async_client/langsmith.async_client.AsyncClient): AsyncClient is a client for interacting with the LangSmith API. It allows users to perform various operations such as creating and managing datasets, prompts, runs, and feedback. The client supports asynchronous operations and provides methods for adding runs to annotation queues, creating commits, and managing projects.
- [Set up threads in LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/threads.md): LangSmith’s Threads feature allows you to track multi-turn conversations in LLM applications by linking related traces together. You can group traces into threads by passing a metadata key like session_id, thread_id, or conversation_id with a unique identifier. The content provides code examples in Python and TypeScript for logging and retrieving conversation history to maintain long-running chats and demonstrates how to view these threads in the LangSmith UI.
- [Create Account and API Key for LangSmith Usage](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/administration/how_to_guides/organization_management/create_account_api_key.md): This document explains how to create an account and an API key for LangSmith. It covers signing up for an account, the differences between Service Keys and Personal Access Tokens, and the steps to create an API key, emphasizing the importance of storing it securely. It also briefly mentions configuring the SDK for the EU instance.
- [Dataset prebuilt JSON schema types](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/data_formats/dataset_json_types.md): LangSmith recommends using JSON schema types for dataset inputs and outputs to ensure data consistency and proper formatting for tasks like evaluations. It offers predefined types like Message and Tool, which can be referenced using JSON Schema references, simplifying LLM workflows.
- [Trace Query Syntax](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/data_formats/trace_query_syntax.md): This document details the filtering capabilities for LangSmith traces, covering arguments like project_id, trace_id, and run_type, as well as a filter query language with comparators like gte, gt, and lte to help analyze and export runs.
- [Email/password (basic auth) authentication in LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/self_hosting/configuration/basic_auth.md): This document outlines the requirements and configuration for using email/password (basic auth) authentication in LangSmith. It details limitations, such as the inability to switch between auth types after installation, and highlights security recommendations for passwords and JWT secrets. The document also provides instructions for migrating from "None" auth and includes configuration examples for both Helm and Docker installations.
- [LangSmith Concepts: Organizations, Workspaces, and Users](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/administration/concepts.md): This guide explains the resource hierarchy in LangSmith, detailing organizations, workspaces, and resource tags. It covers user management, API keys (PATs and service keys), and organization roles, differentiating between organization-level and workspace-level permissions and configurations.
- [Frequently Asked Questions](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/pricing/faq.md): This document provides answers to frequently asked questions about LangSmith, covering topics such as pricing, plan types, billing, usage limits, support, data storage, security compliance, and data training policies.
- [TTL and Data Retention in LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/self_hosting/configuration/ttl.md): This document explains how to configure automatic Time-To-Live (TTL) and data retention for traces in LangSmith Self-Hosted to comply with privacy regulations and manage space efficiently. It details the configuration options via Helm or environment variables, including enabling the feature, and setting retention periods for short-lived and long-lived traces.
- [Administration how-to guides](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/administration/how_to_guides.md): Step-by-step guides for key tasks and operations in LangSmith, covering organization management, workspace setup, billing, access control, and resource tagging.
- [Optimize tracing spend on LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/administration/tutorials/manage_spend.md): This tutorial provides a guide on optimizing spending within LangSmith, focusing on managing data retention settings for traces. It explains how current usage can be monitored through the Usage Graph and Invoices, and details steps to adjust organization-level and project-level data retention policies. The guide also covers implementing server-side sampling for extended data retention to balance cost savings with the need for historical data.`
- [Blob Storage Configuration for LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/self_hosting/configuration/blob_storage.md): This document details how to configure LangSmith to use external blob storage (Amazon S3, Google Cloud Storage, or Azure Blob Storage) for storing run data. It covers requirements, authentication methods for each provider, and configuration steps for Helm and Docker deployments, including options for disabling ClickHouse search.
- [Prompt Engineering Quick Start (SDK)](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/prompt_engineering/quickstarts/quickstart_sdk.md): This guide demonstrates how to use the LangSmith SDK to create, test, and iterate on prompts with LLMs like OpenAI. It covers setup, prompt creation, testing, and iteration, with code examples for both Python and TypeScript.
- [external_redis](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/self_hosting/configuration/external_redis.md): LangSmith leverages Redis for its queuing and caching operations. While it defaults to an internal Redis instance, users can configure LangSmith Self-Hosted to utilize an external Redis instance. This is highly recommended for production environments as it simplifies the management of backups, scaling, and other operational aspects. The guide details the requirements, including supported Redis versions and recommended configurations, and provides instructions on how to format the connection string for both SSL and non-SSL connections. It also explains how to integrate the connection string into the LangSmith configuration, whether using Helm or Docker.
- [External Postgres Database Connection for LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/self_hosting/configuration/external_postgres.md): This document provides a guide on how to connect LangSmith, a distributed tracing and observability platform, to an external PostgreSQL database. It details the requirements for the external database, including version compatibility and necessary extensions, and explains how to construct a proper connection string. The guide also covers configuration steps for both Helm and Docker installations to utilize the external database, emphasizing its recommendation for production environments to aid in managing backups and scaling.
- [Connect to an external ClickHouse database](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/self_hosting/configuration/external_clickhouse.md): This document provides a guide on how to connect LangSmith to an external ClickHouse database. It covers the requirements for setting up ClickHouse, including necessary configuration parameters and version compatibility. The document also explains how to configure LangSmith for both standalone and clustered ClickHouse deployments, including considerations for high availability and load balancing. Finally, it details the parameters needed for LangSmith to connect to an external ClickHouse instance.
- [Self-hosting LangSmith with Docker](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/self_hosting/installation/docker.md): This document provides a comprehensive guide for self-hosting LangSmith using Docker. It covers prerequisites like Docker installation, resource allocation, and obtaining a LangSmith license key. The guide details the process of setting up and running LangSmith via Docker Compose, including fetching configuration files, setting environment variables, starting the server, validating the deployment, and checking logs. It also touches upon post-installation steps such as securing the instance with authentication and SSL.
- [Self-hosting LangSmith on Kubernetes](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/self_hosting/installation/kubernetes.md): This guide provides instructions for deploying LangSmith to a Kubernetes cluster using Helm. It covers prerequisites such as a Kubernetes cluster with sufficient resources, Helm installation, a LangSmith license key, API key salt, and JWT secret. The guide details how to configure Helm charts with essential options like license key, API key salt, and basic authentication settings. It also outlines the deployment steps, including connecting to the Kubernetes cluster, adding the Langchain Helm repo, and finding the latest chart version.
- [Set up resource tags](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/administration/how_to_guides/organization_management/set_up_resource_tags.md): Resource tags are a feature available on Plus and Enterprise plans that allow users to organize resources within a workspace. Tags are key-value pairs that can be created, assigned to workspace-scoped resources (like Tracing Projects, Annotation Queues, Deployments, Experiments, Datasets, and Prompts), and deleted. Users can also filter resources based on these tags to improve organization and navigation within the workspace.
- [Set up access control in LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/administration/how_to_guides/organization_management/set_up_access_control.md): This document explains how to set up access control in LangSmith using Role-Based Access Control (RBAC). It covers creating custom roles with specific permissions and assigning these roles to users within a workspace. RBAC is an enterprise-only feature, with other plans defaulting to an Admin role for all users.
- [Manage Organization by API](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/administration/how_to_guides/organization_management/manage_organization_by_api.md): This document provides a comprehensive guide on managing your organization through LangSmith's API. It outlines programmatic access for UI actions, covering workspaces, user management (RBAC, membership), API keys, and security settings. The guide also includes limitations and sample code for common workflows.
- [Using an Existing Secret for LangSmith Installation in Kubernetes](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/self_hosting/configuration/using_an_existing_secret.md): This document explains how to use existing Kubernetes secrets for your LangSmith installation, which is useful for centralized management of sensitive information. It details the default secrets provisioned by LangSmith, the requirements for using existing secrets, and provides an example Kubernetes secret structure. The document also covers how to configure your LangSmith instance to utilize these pre-existing secrets by modifying the `langsmith_config.yaml` file.
- [User Management in LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/self_hosting/configuration/user_management.md): This document outlines user management features in LangSmith, including organization-level invites managed by workspace admins and options to disable organization or personal organization creation. It covers configuration details for self-hosted deployments via Helm and Docker.
- [Configuring LangSmith for Scale](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/self_hosting/configuration/scale.md): This document outlines the steps and configurations necessary to scale a self-hosted LangSmith instance to handle varying loads, measured in traces per second (TPS). It provides specific hardware and replica recommendations for achieving 10, 100, and 1000 TPS, emphasizing Kubernetes deployments and necessary adjustments to components like Redis, ClickHouse, and various pods. The guide also includes example configuration snippets and important notes on resource allocation and potential bottlenecks.
- [Playground Environment Settings](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/self_hosting/configuration/playground_environment_settings.md): This document explains how to use environment variables to configure model providers in the LangSmith playground. It covers requirements such as a self-hosted LangSmith instance and provider support for environment variables. The configuration section details how to set these variables in Helm charts or Docker Compose files, including examples for OPENAI_BASE_URL and OPENAI_API_KEY.
- [Mirroring Images for LangSmith Installation](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/self_hosting/configuration/mirroring_images.md): This document provides a guide on how to mirror Docker images for a LangSmith installation, which is necessary for environments without internet access or for using a private Docker registry. It details the requirements, including authenticated Docker registry access and Docker installation, and provides instructions on mirroring images using a provided script or manually via Docker commands. The guide also covers the configuration steps needed in either Helm charts or .env files to utilize the mirrored images in LangSmith.
- [Ingress Configuration for LangSmith on Kubernetes](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/self_hosting/configuration/ingress.md): This document explains how to configure an Ingress for a LangSmith installation on Kubernetes to manage custom domains and traffic routing. It details the necessary requirements, optional parameters like hostname, subdomain, and TLS settings, and provides an example configuration within the LangSmith Helm Chart. The guide also covers how to verify the Ingress setup and manually update DNS records if needed.
- [Client](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/python/client/langsmith.client.Client): The Client class in langsmith.client provides a way to interact with the LangSmith API. It allows users to manage and trace language model applications by offering functionalities to create, retrieve, update, and delete various resources like datasets, runs, and prompts. The client also supports features such as data anonymization, input/output hiding, error handling with retries, and integration with OpenTelemetry for enhanced monitoring.
- [Custom TLS Certificates for Model Providers](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/self_hosting/configuration/custom_tls_certificates.md): This document explains how to use custom TLS certificates for model providers in LangSmith, including Azure OpenAI, OpenAI, and custom model servers. It details the environment variables (_LANGSMITH_PLAYGROUND_TLS_MODEL_PROVIDERS, _LANGSMITH_PLAYGROUND_TLS_CA, _LANGSMITH_PLAYGROUND_TLS_KEY, _LANGSMITH_PLAYGROUND_TLS_CERT_) required for configuring custom TLS settings and provides guidance on their usage for enhanced security and custom authentication scenarios.
- [Update Business Information and Invoice Details in LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/administration/how_to_guides/organization_management/update_business_info.md): This document provides a step-by-step guide on how to update your organization's business information, including your invoice email and Tax ID, within LangSmith. It clarifies that these updates are only available for Plus and Startup plans, and details the process for changing invoice recipients and business-related tax information.
- [TracerSession](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/python/schemas/langsmith.schemas.TracerSession): TracerSession is a Python class representing a tracing session in LangSmith. It includes details such as session ID, start and end times, description, name, and tenant ID. It also supports metadata, tags, and dataset references, and provides methods for copying, exporting to dictionaries or JSON, and validating data.
- [_runner](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/python/_modules/langsmith/evaluation/_runner.md): This Python module provides the V2 Evaluation Interface for LangSmith, enabling users to evaluate target systems (functions, Runnables, or existing experiments) on datasets. It supports various evaluators, concurrent execution, and detailed result reporting, facilitating hiệu quả system performance analysis and comparison.
- [ComparativeExperimentResults](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/python/evaluation/langsmith.evaluation._runner.ComparativeExperimentResults): Represents the results of an evaluate_comparative() call, providing an iterator interface to access experiment results as they become available. It includes methods to retrieve the experiment name and wait for processing.
- [ExperimentResults - LangSmith documentation](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/python/evaluation/langsmith.evaluation._runner.ExperimentResults): Represents the results of an evaluate() call in LangSmith. This class provides an iterator interface to go over the experiment results as they become available and methods to access the experiment name, the number of results, and to wait for the results to be processed.
- [Setting Up Billing for LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/administration/how_to_guides/organization_management/set_up_billing.md): This document provides a comprehensive guide on how to set up billing for your LangSmith account. It covers different scenarios, including setting up billing for personal organizations on the Developer plan, shared organizations on the Plus plan, and accounts created before the introduction of pricing. The guide includes step-by-step instructions with visual aids for adding credit card information and managing organization members.
- [ExampleSearch Schema](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/python/schemas/langsmith.schemas.ExampleSearch): ExampleSearch is a Python class from the LangSmith library used for searching examples. It includes fields for dataset ID, inputs, outputs, metadata, and a unique ID, with methods for creating, copying, and converting the data to dictionary or JSON formats.
- [PromptCommit Schema](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/python/schemas/langsmith.schemas.PromptCommit): Represents a Prompt with a manifest, including owner, repository, commit hash, manifest details, and examples. This class is used for creating and managing prompt versions within LangSmith.
- [Set up Workspace in LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/administration/how_to_guides/organization_management/set_up_workspace.md): This document guides users on setting up and managing workspaces in LangSmith. It covers creating new workspaces, managing user access and roles with RBAC, configuring workspace settings like API keys and models, and the process of deleting workspaces, with a warning about data loss. Workspaces are used to isolate resources and manage permissions between different teams.
- [Set up an organization](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/administration/how_to_guides/organization_management/set_up_organization.md): This document provides a guide on how to set up and manage organizations and workspaces within LangSmith. It covers creating a new organization, managing users and their roles, and navigating between different workspaces for collaboration and resource management.
- [Run - Langsmith Schema](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/python/schemas/langsmith.schemas.Run): The Run class in langsmith.schemas defines the data schema for a single execution trace, including start/end times, inputs/outputs, token/cost metrics, and metadata like tags and attachments, essential for managing and analyzing language model application executions.
- [ListPromptsResponse schema for LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/python/schemas/langsmith.schemas.ListPromptsResponse): This document outlines the ListPromptsResponse class in Langsmith, detailing its attributes like repos and total, and providing methods for initialization, copying, and data conversion.
- [ListedPromptCommit Schema](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/python/schemas/langsmith.schemas.ListedPromptCommit): Represents a prompt commit in LangSmith, including its unique ID, owner, repository, and other relevant metadata such as commit hash and timestamps.
- [RunWithAnnotationQueueInfo Schema](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/python/schemas/langsmith.schemas.RunWithAnnotationQueueInfo): A Python class representing run data with annotation queue information, including details like ID, name, timestamps, run type, and optional fields for errors, inputs, outputs, and attachments.
- [Prompt](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/python/schemas/langsmith.schemas.Prompt): Represents a Prompt with metadata, including repository details, creation information, and usage statistics.
- [FeedbackIngestToken Schema](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/python/schemas/langsmith.schemas.FeedbackIngestToken): Represents the schema for a feedback ingest token, including its ID, URL, and expiration time. This class provides methods for creating, copying, and converting the token data to dictionary or JSON formats.
- [Feedback Schema](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/python/schemas/langsmith.schemas.Feedback): Provides a schema for feedback, including attributes like an ID, timestamps, run/trace IDs, key, score, value, comments, corrections, feedback sources, session IDs, and comparative experiment IDs.
- [Example](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/python/schemas/langsmith.schemas.Example): The Example object represents a single data point within a LangSmith dataset, including inputs, outputs, and associated metadata, and is used for versioning and managing test cases.
- [Dataset](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/python/schemas/langsmith.schemas.Dataset): A Pydantic model representing a dataset in LangSmith, including its name, description, data type, creation details, and schema information for inputs and outputs, with support for transformations.
- [AnnotationQueue](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/python/schemas/langsmith.schemas.AnnotationQueue): Represents an annotation queue in LangSmith. It includes attributes like ID, name, description, creation/update timestamps, and tenant ID.
- [Async Client for LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/python/_modules/langsmith/async_client.md): The Async LangSmith Client module provides an asynchronous interface for interacting with the LangSmith API. It allows users to perform operations like creating, retrieving, and managing language model traces and datasets without blocking the execution thread. The client handles API requests, responses, and retries, ensuring efficient and robust communication with the LangSmith platform.
- [_expect](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/python/_expect.md): The _expect module in LangSmith allows users to create approximate assertions, known as "expectations," on test results within test cases. It is designed for use with the @pytest.mark.decorator and facilitates logging scores and making assertions that are recorded as "expectation" feedback in LangSmith. The module provides functionalities for comparing predictions with references using methods like embedding distance and edit distance, and also supports direct value assertions and custom checks.
- [testing](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/python/testing.md): LangSmith pytest testing module provides functionalities to log run feedback, inputs, outputs, and reference outputs from within a pytest test run. It also includes a utility to trace pytest test cases as runs in LangSmith.
- [Troubleshooting LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/self_hosting/troubleshooting.md): This guide provides instructions and solutions for common issues encountered when running a self-hosted LangSmith instance, covering aspects like gathering debugging information (logs, browser HAR files) for Kubernetes and Docker setups, and resolving specific errors such as database exceptions, request size limitations, and permission issues.
- [LangSmith-managed ClickHouse](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/self_hosting/langsmith_managed_clickhouse.md): LangSmith uses Clickhouse for storing traces and feedback. For easier management and scaling, it is recommended to connect a self-hosted LangSmith instance to an external Clickhouse instance. LangSmith-managed ClickHouse is an option that allows you to use a fully managed ClickHouse instance that is monitored and maintained by the LangSmith team. Sensitive information (inputs and outputs) of your traces will be stored in cloud object storage (S3 or GCS) within your cloud instead of Clickhouse.
- [Frequently Asked Questions for LangSmith Self-Hosting](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/self_hosting/faq.md): This document provides answers to frequently asked questions regarding the self-hosting of LangSmith, covering topics such as API key and user management, load balancing, authentication via SSO, external storage configurations, egress requirements, and resource specifications for different deployment methods.
- [LangSmith Release Notes](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/self_hosting/release_notes.md): The LangSmith release notes cover updates from October 2024 (v0.8), August 2024 (v0.7), and June 2024 (v0.6). Key new features include resource tags, synthetic data generation, enhanced trace comparison, dataset schema management, multiple annotators, run filtering, improved search, webhook notifications, playground comparisons, and custom prompt storage. Performance improvements were made to the Threads view and error handling. Infrastructure changes include TTL settings, blob storage support, default resource allocations, Redis persistence, updated Clickhouse migrations, deprecation of an API domain, and health checks. Admin changes introduced password authentication, disabled personal organizations, and allowed workspace admins to add users. Notable deprecations include the LangChain Hub SDK and older LangSmith versions.
- [Egress Configuration for LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/self_hosting/egress.md): This document outlines the egress requirements for self-hosted LangSmith instances, specifically the need for egress to https://beacon.langchain.com for operational metadata collection and remote support. It details the types of data collected, including subscription metrics and operational metadata, and provides example payloads for license verification and usage reporting. The document also assures users of LangChain's commitment to data privacy and security.
- [General Upgrade Instructions for LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/self_hosting/upgrades.md): This document provides instructions for upgrading LangSmith via Kubernetes (Helm) and Docker. It details the necessary commands for adding repositories, updating charts, and verifying successful deployment for both methods. Specific steps include updating Helm chart configurations, using `helm upgrade`, and checking pod status, as well as modifying Docker Compose files and environment variables for Docker upgrades.
- [Using Self-Hosted LangSmith Instance](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/self_hosting/usage.md): This guide explains how to use a self-hosted LangSmith instance, including configuring applications to connect to it via environment variables or the SDK. It also provides details on accessing the API reference.
- [Configuration](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/self_hosting/configuration.md): This document provides a comprehensive guide to configuring LangSmith instances, covering essential areas such as authentication, user management, database connections, data retention, blob storage, TLS certificates, ingress, and scaling.
- [Scripts for administering LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/self_hosting/scripts.md): This document provides a list of administrative scripts for LangSmith, including instructions for deleting organizations, workspaces, and traces, as well as scripts for generating statistics and running support queries for Clickhouse and Postgres. The scripts are available in the Helm chart repository.
- [Architectural Overview of Self-Hosted LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/self_hosting/architectural_overview.md): This document provides a comprehensive overview of the architecture for self-hosted LangSmith. It details the various components, including frontend, backend, platform backend, playground, queue, and ACE backend, as well as the essential storage services: ClickHouse, PostgreSQL, and Redis. The document also covers how to expose the LangSmith UI and recommends using external storage services for production environments.
- [SSO with OAuth2.0 and OIDC](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/self_hosting/configuration/sso.md): This document explains how to set up Single Sign-On (SSO) for LangSmith Self-Hosted using OAuth2.0 and OIDC. It covers configuration details for both "with Client Secret" (recommended) and "without Client Secret" (deprecated) flows, including requirements and identity provider setup examples for Google Workspace. The guide details necessary parameters, security considerations, and potential upgrade paths.
- [Set Up SAML SSO for LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/administration/how_to_guides/organization_management/set_up_saml_sso.md): This document provides a comprehensive guide for Enterprise Cloud customers on setting up SAML Single Sign-On (SSO) for LangSmith. It details the benefits of SSO, prerequisites for configuration, and step-by-step instructions for integrating with Identity Providers (IdPs) like Entra ID, Google, and Okta. The guide also covers Just-in-Time (JIT) provisioning, login method enforcement, and troubleshooting common issues.
- [Feedback Data Format with LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/data_formats/feedback_data_format.md): This document outlines the data format for feedback in LangSmith, detailing its fields, types, and purpose. It explains how feedback is stored, from user annotations to automated evaluations, and provides an example JSON representation.
- [Run Data Format](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/data_formats/run_data_format.md): LangSmith offers a standardized format for storing and processing trace data, facilitating easy export and import. The format includes fields like ID, name, inputs, run type, start/end times, and outputs, with specific emphasis on 'id', 'name', 'inputs', 'run_type', 'start_time', and 'end_time' for key information. It also details the 'dotted_order' field, a sortable key that defines a run's hierarchical position within tracing, derived from start times and run IDs.
- [Trace JS functions in serverless environments](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/serverless_environments.md): This guide explains how to trace JavaScript functions in serverless environments using LangSmith, ensuring all tracing data is flushed before execution terminates. It covers setting environment variables, using awaitPendingTraceBatches, and managing rate limits with manual flushing.
- [LangSmith Collector-Proxy Overview](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/collector_proxy.md): The LangSmith Collector-Proxy is a middleware service for efficient aggregation, compression, and bulk-uploading of OTEL tracing data to LangSmith. It is optimized for parallel environments generating high volumes of spans and supports OTLP input, flexible batching, and semantic translation. Configuration is primarily managed through environment variables.
- [Configuring Webhook Notifications for LangSmith Alerts](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/alerts_webhook.md): This guide explains how to set up webhook notifications for LangSmith alerts, enabling integration with custom services and third-party platforms. It covers prerequisites, configuration steps including URL, headers, and request body templates, testing, troubleshooting, and security considerations. The guide also provides a detailed example for sending alerts to Slack using the chat.postMessage API, including Slack app creation and permission configuration.
- [PagerDuty Integration for LangSmith Alerts](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/alerts_pagerduty.md): This guide explains how to integrate LangSmith alerts with PagerDuty using the Events API v2. It covers creating a PagerDuty service, obtaining the integration key, and configuring LangSmith to send alerts to PagerDuty. The guide also provides troubleshooting tips and links to relevant documentation.
- [Alerts in LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/alerts.md): LangSmith allows users to set up alerts to proactively identify critical issues in their LLM applications, such as API rate limit violations, increased latency, and performance regressions. Users can configure alerts by navigating to the project, selecting a metric type (Errored Runs, Feedback Score, or Latency), defining alert conditions with aggregation methods and thresholds, and choosing a notification channel like PagerDuty or webhooks.
- [Bulk Exporting Trace Data](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/data_export.md): This document outlines LangSmith's bulk data export functionality, enabling users to export trace data to external destinations like BigQuery, Snowflake, and Jupyter Notebooks. It details the process of setting up S3 buckets as destinations, including required configurations for AWS S3 and compatible storage like Google Cloud Storage. The guide also covers creating export jobs with specific date ranges and filters, and monitoring their status.
- [Troubleshoot Trace Nesting with LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/nest_traces.md): This document provides solutions to common issues where child runs in LangSmith traces appear in separate traces instead of nesting correctly. It covers problems related to context propagation in Python, specifically with asyncio and threading, and offers solutions like upgrading Python versions or manually propagating context using techniques like passing parent run trees or using LangSmith's ContextThreadPoolExecutor.
- [Calculate Token-Based Costs with LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/calculate_token_based_costs.md): This document explains how to calculate token-based costs for LLM runs in LangSmith. It covers methods for sending token counts, specifying model names, and setting model prices, including details on pricing breakdowns and a cost calculation formula. It also briefly mentions direct cost submission.
- [Trace with OpenAI Agents SDK](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/trace_with_openai_agents_sdk.md): This document provides instructions on how to integrate LangSmith tracing with the OpenAI Agents SDK. It covers installation and a quick start guide with a code example demonstrating how to trace an agent's execution flow.
- [Trace using the LangSmith REST API](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/trace_with_api.md): This document provides a guide on how to trace requests using the LangSmith REST API, recommending the use of Python or TypeScript SDKs for optimal performance. It details how to log runs using the POST and PATCH /runs endpoints, emphasizing the need to include the API key in request headers and explaining automatic generation of trace and dotted order fields for simpler, albeit slower, logging. The guide also covers batch ingestion using the POST /runs/multipart endpoint for faster data transfer and higher rate limits, including helper functions for constructing and serializing run data.
- [Trace without setting environment variables](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/trace_without_env_vars.md): This document explains how to configure LangSmith tracing programmatically when environment variables cannot be set. It covers using the `tracing_context` in Python and passing `tracingEnabled` in TypeScript, with examples for both languages.
- [Trace with the Vercel AI SDK](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/trace_with_vercel_ai_sdk.md): This document provides instructions on how to trace runs from the Vercel AI SDK using LangSmith's AISDKExporter, with code examples for Next.js, Node.js, Sentry, and Cloudflare Workers.
- [Running Support Queries Against Postgres](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/self_hosting/scripts/running_pg_support_queries.md): This document provides a guide on how to run SQL queries against a PostgreSQL database to retrieve data not directly supported by the LangSmith UI, such as multi-organization trace counts. It details the necessary prerequisites, including tools like kubectl and PostgreSQL clients, and database connection details. The guide also explains how to use a provided script to execute queries, with examples for connecting to a local PostgreSQL instance and outputting results to a CSV file.
- [Generating Query Stats for LangSmith Troubleshooting](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/self_hosting/scripts/generate_query_stats.md): This document provides a guide on how to generate LangSmith query statistics for troubleshooting purposes. It outlines the necessary prerequisites, including kubectl, Clickhouse database credentials, and network connectivity. The document also details the script and command needed to generate a CSV file containing these statistics, which can then be shared with the LangChain team.
- [Generating Clickhouse Stats](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/self_hosting/scripts/generate_clickhouse_stats.md): This document provides a guide on how to generate Clickhouse statistics for troubleshooting self-hosted LangSmith instances. It details the prerequisites, including kubectl and Clickhouse credentials, and explains how to run a script to obtain a CSV file containing memory and CPU consumption, and connection concurrency data.
- [Deleting Traces in LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/self_hosting/scripts/delete_traces.md): This document provides instructions on how to delete traces in LangSmith using a provided script. It details the prerequisites, including kubectl and Clickhouse database credentials, and explains how to run the script for single or multiple trace IDs. The process involves directly removing traces from ClickHouse materialized views and tables.
- [Deleting Organizations in LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/self_hosting/scripts/delete_an_organization.md): This document provides a guide on how to delete an organization from a self-hosted LangSmith instance. It outlines the prerequisites, including necessary tools like kubectl and PostgreSQL client, and database credentials for both PostgreSQL and ClickHouse. The guide details how to establish connectivity to these databases, often requiring port-forwarding, and provides the command to run the deletion script with organization ID as an argument. It includes an example command for a bundled version with port-forwarding.
- [Deleting Workspaces in LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/self_hosting/scripts/delete_a_workspace.md): This document provides instructions on how to delete a workspace in LangSmith. It covers the native support in Self-Hosted v0.10 and provides a detailed guide for older versions, including prerequisites like kubectl and a PostgreSQL client, and necessary database credentials. It also explains how to run a deletion script with example commands.
- [Running Support Queries against Clickhouse](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/self_hosting/scripts/running_ch_support_queries.md): This document provides a guide on how to run support queries against a Clickhouse database using provided scripts. It details the prerequisites, including kubectl and Clickhouse credentials, and explains how to execute the query script with examples. The script helps in obtaining unsupported query outputs like exception logs from Clickhouse.
- [Trace with OpenTelemetry](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/trace_with_opentelemetry.md): This guide demonstrates how to log traces from OpenTelemetry clients to LangSmith. It covers installation, environment configuration for OpenTelemetry endpoint and headers, and provides a Python code example using the OpenAI client to send traces with relevant attributes. It also details the mapping between OpenTelemetry attributes and LangSmith fields for core, GenAI standard, and GenAI request parameters.
- [Trace with Instructor Integration for LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/trace_with_instructor.md): This document details how to integrate LangSmith with the Instructor library in Python. It covers setting up the LangSmith API key, installing the LangSmith SDK, wrapping the OpenAI client, and patching it with Instructor. The guide also provides examples of how to log LLM calls made with Instructor to LangSmith, including how to create nested traces using the @traceable decorator for functions that use the wrapped client.
- [Trace generator functions](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/trace_generator_functions.md): This document explains how to use generator functions with LangSmith's tracing functionality to stream outputs and minimize the time to the first token. It provides examples in Python and TypeScript for both synchronous and asynchronous generators, and demonstrates how to customize output aggregation using the `aggregate` option.
- [Share or unshare a trace publicly](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/share_trace.md): This document provides instructions on how to share or unshare a trace publicly on LangSmith. It explains the implications of public sharing, security considerations for self-hosted deployments, and the step-by-step process for both sharing and unsharing traces via direct links or through the organization's shared URLs list.
- [Query Traces in LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/export_traces.md): This document explains how to query traces in LangSmith using both simple filter arguments and the more complex query language. It covers listing runs by project, run type, root status, errors presence, and specific IDs, as well as advanced filtering based on metadata, feedback, latency, token usage, timestamps, and search terms.
- [Prevent logging of sensitive data in traces | LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/mask_inputs_outputs.md): LangSmith allows you to prevent sensitive data from being logged in traces by using environment variables or by customizing the Client instance. You can hide all inputs and outputs or use a custom anonymizer with regex patterns or a function to mask specific data like email addresses and UUIDs.
- [Log custom LLM traces](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/log_llm_trace.md): This document explains how to log custom LLM traces in LangSmith to ensure proper processing and rendering, including token counting and cost calculation. It details the required format for inputs and outputs, providing examples for chat-style models and discussing different output formats like messages, choices, tuples, and direct message objects.
- [Log retriever traces](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/log_retriever_trace.md): Learn how to log retriever traces in LangSmith for better visualization and debugging.LangSmith provides special rendering for retrieval steps in traces.Annotate the retriever step with run_type="retriever" and return a list of dictionaries, each containing page_content, type, and metadata.
- [Log multimodal traces](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/log_multimodal_traces.md): LangSmith enables logging and viewing images within traces for multimodal LLM runs. The documentation provides Python and TypeScript examples using wrapped OpenAI clients to log images via URLs or base64 encoding, which are then rendered in the LangSmith UI.
- [Distributed Tracing in LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/distributed_tracing.md): LangSmith facilitates distributed tracing by propagating trace context across services using headers. It provides examples for Python and TypeScript, demonstrating how to implement tracing in client-server applications using frameworks like FastAPI, Starlette, Express.js, and Hono.
- [Accessing the Current Run (Span) in LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/access_current_span.md): Learn how to access the current run (span) within a traced function using LangSmith's Python and TypeScript SDKs. This allows you to retrieve run UUIDs, tags, and other relevant information by calling `get_current_run_tree` or `getCurrentRunTree`.
- [Add metadata and tags to traces](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/add_metadata_tags.md): LangSmith allows you to associate metadata and tags with traces for categorization and additional information. Tags are strings for labeling, while metadata is a dictionary of key-value pairs. This helps in tracking information like execution environment or correlation IDs. The documentation provides guidance on using tags and metadata for filtering traces and includes examples for both Python and TypeScript.
- [Automating LangSmith Rules](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/rules.md): This document provides a comprehensive guide on setting up automation rules in LangSmith. It details how to define rules with filters, sampling rates, and actions, such as adding traces to datasets, triggering evaluations, or sending webhooks. The guide also covers applying rules to past runs and viewing automation logs for monitoring and troubleshooting.
- [Log traces to specific project](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/log_traces_to_project.md): This document details how to log traces to a specific project in LangSmith. It covers both static configuration using environment variables (e.g., LANGSMITH_PROJECT) and dynamic configuration at runtime via code annotations. The document provides examples for Python and TypeScript, demonstrating how to set project names using the @traceable decorator, wrapped clients, and RunTree objects, with dynamic settings overriding static ones.
- [Upload files with traces](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/upload_files_with_traces.md): This document explains how to upload binary files, such as images, audio, videos, PDFs, and CSVs, along with your traces using the LangSmith SDK. It covers both Python and TypeScript examples, demonstrating how to use the Attachment type in Python and Uint8Array/ArrayBuffer in TypeScript to include file attachments in your traces.
- [Filter traces in the application](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/filter_traces_in_application.md): This document provides a guide on how to filter traces within the LangSmith application. It covers filtering by run attributes, time range, and specific techniques like filtering for intermediate runs, content in inputs/outputs, and key-value pairs. The guide also includes examples for filtering tool calls.
- [Annotate code for tracing](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/annotate_code.md): This document explains how to annotate code for tracing using LangSmith. It covers using the `@traceable` decorator and `traceable` function, the `trace` context manager in Python, and wrapping the OpenAI client. It also mentions environment variables required for tracing and how to log traces to different projects.
- [Observability how-to guides | LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides.md): This document provides step-by-step guides for adding observability to LLM applications using LangSmith. It covers tracing configuration, project UI and API interactions, monitoring, automations, and human feedback mechanisms.
- [Observability in LLM Applications with LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/tutorials/observability.md): This page explains how to add observability to LLM applications using LangSmith. It covers setting up the environment, tracing LLM calls, and tracing the entire RAG pipeline for better debugging and iteration during development.
- [Online Evaluations in LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/online_evaluations.md): This document provides a guide on setting up online evaluations in LangSmith. It explains two types of online evaluations: LLM-as-a-judge and Custom Code. The guide details the steps for configuring these evaluations, including writing custom Python code and testing it within the platform.
- [Evaluation concepts](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/concepts.md): This guide explains the LangSmith evaluation framework and AI evaluation techniques. It covers datasets, which are collections of test inputs and reference outputs, and evaluators, which are functions for scoring outputs. The guide also details various methods for dataset curation, including manual, historical, and synthetic data, as well as dataset splits and versioning. It further elaborates on evaluator inputs, outputs, and definition methods, including custom and built-in evaluators.
- [Vitest/Jest Evals for LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/vitest_jest.md): This document details how to integrate LangSmith with Vitest and Jest for running evaluations. It covers installation, setup for both Vitest and Jest, and defining/running evaluation tests using familiar syntax. The integration allows JavaScript and TypeScript developers to define datasets and evaluate them within their existing testing frameworks, with specific guidance on configuration and test case structure for seamless integration with LangSmith's dataset and experiment tracking.
- [Summary Evaluator Example](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/summary.md): This document provides an example of how to define and use summary evaluators in LangSmith. It explains that summary evaluators compute metrics across an entire experiment, unlike basic evaluators that work on individual runs. The document includes Python and TypeScript code examples for a F1-score summary evaluator and demonstrates how to integrate it with the LangSmith client's evaluate method.
- [Repetitions in LangSmith Evaluation](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/repetition.md): This document explains how to use repetitions in LangSmith evaluations to get more accurate performance estimates for language models. It details how to configure the number of repetitions in experiments and how to view and interpret the results, including average scores and individual run data.
- [Handling Model Rate Limits](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/rate_limiting.md): This document provides strategies for managing third-party API rate limits, particularly when running large evaluation jobs. It covers using Langchain RateLimiters for client-side control, implementing retries with exponential backoff, and limiting max concurrency to avoid rate limiting errors.
- [Return Multiple Scores in LangSmith Evaluators](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/multiple_scores.md): This document explains how to return multiple metrics from custom or summary evaluator functions in LangSmith using the Python or JavaScript SDKs. It details the required data format for returning numerical and categorical scores, providing code examples for both.
- [wrap_anthropic LangSmith Wrapper](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/python/wrappers/langsmith.wrappers._anthropic.wrap_anthropic): This function patches the Anthropic client to make it traceable, allowing for integrated tracing of Anthropic API calls within the LangSmith platform. It accepts an Anthropic client and optional tracing extras, returning the patched client for use in applications. The documentation also provides examples of how to use the wrapped client for both standard and streaming requests.
- [multi_turn_simulation](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/multi_turn_simulation.md): This guide explains how to simulate multi-turn interactions for evaluating conversational AI applications. It covers setting up the environment, defining the application logic and a simulated user, and running the simulation using the openevals package. The example demonstrates a customer support scenario with an aggressive user and a patient agent, highlighting the process of dynamic trajectory creation and evaluation.
- [Custom Evaluators: Categorical vs Numerical Metrics](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/metric_type.md): This document explains how to return categorical and numerical metrics when writing custom evaluators in LangSmith. It details the required formats for returning metrics as integers, floats, booleans, or strings, and provides examples in both Python and TypeScript.
- [How to run an evaluation locally](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/local.md): This document explains how to run evaluations locally using the LangSmith Python SDK with `upload_results=False`. It provides a practical code example demonstrating how to define a dataset, evaluator, and application, run the evaluation, and analyze the results without uploading them to LangSmith.
- [LLM-as-a-judge Evaluator Guide](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/llm_as_judge.md): This guide explains how to define an LLM-as-a-judge evaluator for offline evaluation using the LangSmith SDK or UI. It covers pre-built evaluators, creating custom evaluators with Python code, and customizing prompts, models, and feedback configurations. Enhanced with few-shot examples for better alignment with human preferences.
- [Evaluating Langgraph Graphs](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/langgraph.md): This document provides a guide on evaluating Langgraph graphs, which are used for building stateful, multi-actor applications with LLMs. It covers end-to-end evaluations, defining graphs, creating datasets, setting up evaluators, and running evaluations using LangSmith's aevaluate function. The guide also touches upon evaluating intermediate steps within the graph execution.
- [How to evaluate a Langchain Runnable](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/langchain_runnable.md): This document explains how to evaluate Langchain Runnable objects, such as chains and models, using the LangSmith platform. It provides setup instructions for both Python and TypeScript, including code examples for defining a chain and evaluating it with a toxicity detection dataset. The guide also shows how to interpret evaluation results and includes links to related topics like evaluating LangGraph.
- [Filter Experiments in LangSmith UI](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/filter_experiments_ui.md): LangSmith allows users to filter experiments based on feedback scores and metadata, making it easier to find specific experiments. The document explains how to add metadata to experiments using the SDK and demonstrates how to apply and stack filters in the UI to narrow down results. It also covers how to reset filters to explore different experiment parameters.
- [Run an evaluation with attachments](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/evaluate_with_attachments.md): This document explains how to run evaluations with multimodal content on LangSmith. It details the benefits of using attachments for file types like images and audio, and provides code examples in both Python and TypeScript for creating and uploading datasets with attachments. It also covers how to define a target function that can consume these attachments during evaluation.
- [Define Target Function for LangSmith Evaluation](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/define_target.md): This guide explains how to define a target function for evaluating applications in LangSmith. It covers the function signature, automatic tracing, and provides examples for single LLM calls, non-LLM components like tools, and complete agentic applications. The target function takes dataset inputs and returns application outputs, which are then scored by evaluators.
- [How to evaluate on a specific dataset version](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/dataset_version.md): This document explains how to evaluate a model on a specific version of a dataset using LangSmith. It details how to use the `list_examples` method with the `as_of` parameter to fetch examples from a particular dataset version. The guide also includes Python and TypeScript code examples demonstrating the process. It is recommended to read the guides on dataset versioning and fetching examples before proceeding.
- [Evaluate on a Split or Filtered Dataset View](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/dataset_subset.md): This document provides instructions on how to evaluate a LangSmith dataset using a filtered subset or specific splits. It details using the `list_examples` method with metadata filtering and split selection in both Python and TypeScript. The guide also includes links to related documentation for fetching dataset views and managing dataset splits.
- [Custom Evaluator in LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/custom_evaluator.md): LangSmith allows defining custom evaluators as functions that take dataset examples and application outputs to return metrics. These functions can accept arguments like run, example, inputs, outputs, and reference outputs, returning various data types such as dictionaries, numbers, strings, or lists of dictionaries to represent evaluation results.
- [How to run an evaluation asynchronously using LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/async.md): This document explains how to run asynchronous evaluations using LangSmith's aevaluate() function in Python. It covers setting up an asynchronous application, defining evaluator functions, and running the evaluation with optional concurrency. It also provides a complete code example demonstrating these steps.
- [Uploading Experiments with the REST API](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/upload_existing_experiments.md): This guide explains how to upload experiments run outside of LangSmith using the REST API, detailing the request body schema and providing an example request for the /datasets/upload-experiment endpoint.
- [EvaluateOptions](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/js/interfaces/evaluation.EvaluateOptions): This interface defines the options for evaluating datasets, including specifying the data, evaluators, and experiment settings like concurrency and repetition.
- [vitest.describe](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/js/functions/vitest.describe): The describe function in LangSmith is a wrapper for Vitest tests, allowing you to group tests and configure their execution with options like concurrent, only, and skip. It takes a name, a callback function, and an optional configuration object.
- [run_evals_api_only](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/run_evals_api_only.md): This document explains how to use the LangSmith REST API to run evaluations, focusing on creating datasets and running experiments with different language models. It provides Python code examples using the requests library to interact with the API for tasks like creating datasets, running LLM completions, and managing experiments.
- [Fetch Performance Metrics for an Experiment](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/fetch_perf_metrics_experiment.md): This document explains how to fetch performance metrics for experiments in LangSmith. It clarifies that experiments, projects, and sessions refer to the same backend data structure. The document provides code examples in Python and TypeScript to demonstrate fetching metrics like latency, token usage, costs, and feedback statistics using the LangSmith SDK. It also includes prerequisites like creating a dataset and running an experiment.
- [Create Few-Shot Evaluators with LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/create_few_shot_evaluators.md): Learn how to improve LLM-as-a-judge evaluators with few-shot examples in LangSmith. This guide explains how few-shot examples align evaluator outputs with human preferences by incorporating corrections into the evaluator prompt. It covers configuring variable mappings, specifying the number of examples, and viewing the corrections dataset.
- [aevaluate](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate): Evaluates an asynchronous target system on a given dataset, supporting row-level and summary evaluators, concurrency control, and experiment tracking.
- [Audit Evaluator Scores in LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/audit_evaluator_scores.md): LangSmith allows users to manually audit and correct scores provided by LLM-as-a-judge evaluators through its UI or SDK. This guide explains how to make these corrections in the comparison view, runs table, or via the SDK by updating feedback with a score and an optional explanation.
- [Pairwise Evaluations in LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/evaluate_pairwise.md): LangSmith enables comparative evaluations of existing experiments, allowing you to score multiple outputs against each other. It supports this through the `evaluate()` function, which can compare two experiments. The document details how to define pairwise evaluators using functions with specific arguments like inputs and outputs, and how these evaluators should return scores in a dictionary or list format. It also provides Python and TypeScript examples of running pairwise evaluations, including using a pre-defined prompt for ranked preference.
- [pytest](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/pytest.md): A guide to running LangSmith evaluations with pytest, covering installation, defining tests, logging inputs/outputs/feedback, and tracking results in datasets and experiments. It highlights features like automatic syncing, assertion errors in CI, and pytest-like outputs, with examples for SQL generation and feedback logging.
- [Exporting Filtered Traces from LangSmith Experiments to Datasets](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/export_filtered_traces_to_dataset.md): This guide explains how to export filtered traces that meet specific evaluation criteria from a LangSmith experiment into a dataset. It covers navigating to experiment traces, applying filters based on scores or other criteria, and adding the selected traces to a dataset.
- [Use Off-the-Shelf Evaluators in LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/use_langchain_off_the_shelf_evaluators_old.md): This document explains how to use LangSmith's pre-built evaluators to assess application performance without custom code. It covers question/answer correctness, criteria-based evaluation, labeled criteria, string metrics, and embedding distance, along with using custom LLMs.
- [Share Dataset](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/share_dataset.md): This document explains how to share and export datasets from LangSmith. It details the process for making a dataset publicly accessible via a link and how to unshare it. It also covers exporting datasets in various formats like CSV, JSONL, and OpenAI's fine-tuning format.
- [Annotation Queues](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/annotation_queues.md): Annotation queues in LangSmith allow directed human feedback on specific runs. You can create queues, define feedback criteria, and manage collaborators. Runs can be assigned to queues manually, in bulk, or via automation rules. Reviewers interact with a focused view to provide feedback, attach comments, and score runs, streamlining the quality assurance process.
- [Dataset Versioning in LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/version_datasets.md): LangSmith automatically versions datasets, creating a new version timestamped with each modification (add, update, delete). Users can view past versions to see historical states and operations, with examples being read-only in older versions. Versions can be semantically tagged (e.g., "prod") using the UI or SDK to mark significant milestones, facilitating targeted testing and evaluation.
- [Client](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/js/classes/client.Client): The Client class in LangSmith SDK version 0.3.33 provides an interface for interacting with LangSmith resources, including creating, reading, updating, and deleting datasets, projects, prompts, and runs.
- [Evaluate On Intermediate Steps](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/evaluate_on_intermediate_steps.md): This document explains how to evaluate intermediate steps in an application pipeline, using a retrieval-augmented generation (RAG) example. It covers defining the pipeline, creating a dataset with examples, and specifies the use of custom evaluators for retrieval and LLM-based evaluators for generation.
- [evaluate](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/js/functions/evaluation.evaluate): The evaluate function in LangSmith SDK (v0.3.33) is used for evaluating language models. It supports both comparative evaluations with ComparativeTargetT and ComparativeEvaluateOptions, returning ComparisonEvaluationResults, and standard evaluations with StandardTargetT and EvaluateOptions, returning ExperimentResults.
- [Annotate traces and runs inline](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/annotate_traces_inline.md): LangSmith enables manual annotation of traces with feedback, either inline or via an annotation queue. Feedback can be attached to any run (span) within a trace, not just the root. To annotate inline, click "Annotate" in the trace view. This opens a pane to select feedback tags, assign scores, and add comments, with options to set up new feedback criteria directly.
- [Set up feedback criteria](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/set_up_feedback_criteria.md): This document explains how to set up feedback criteria in LangSmith. It covers both continuous feedback, where a range of values is accepted, and categorical feedback, where predefined categories are mapped to scores. The process involves creating new tags in the LangSmith workspace settings.
- [Running an evaluation from the prompt playground](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/run_evaluation_from_prompt_playground.md): LangSmith allows users to run evaluations directly in the prompt playground without writing any code. Users need an existing dataset to run an evaluation. The process involves navigating to the playground, adding a prompt, selecting a dataset, starting the experiment, and viewing the results. Users can also add evaluators like LLM-as-a-judge or custom code evaluators to assess specific criteria.
- [manage datasets programmatically](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/manage_datasets_programmatically.md): This document outlines how to programmatically manage datasets using LangSmith's Python and TypeScript SDKs. It covers creating datasets from lists of values, traces, CSV files, and pandas DataFrames, as well as fetching existing datasets.
- [Bind Evaluator to Dataset](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/bind_evaluator_to_dataset.md): This document provides a guide on how to bind evaluators to datasets using the LangSmith UI. It explains the process for both LLM-as-judge and custom code evaluators, detailing the steps involved, input variables, and expected outputs. The guide also covers how newly configured evaluators affect subsequent experiment runs and how to view the results.
- [Manage Datasets in Application](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/manage_datasets_in_application.md): This document provides a comprehensive guide on creating and managing datasets within the LangSmith application. It covers various methods, including manual and automatic creation from tracing projects, using annotation queues, the Prompt Playground, and importing data from CSV or JSONL files. The guide also details how to manage datasets by creating schemas, applying transformations, and managing dataset splits, emphasizing repeatable evaluations and consistent data management.
- [How to run LangSmith evaluations](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/evaluate_llm_application.md): This guide explains how to evaluate an application using the evaluate() and aevaluate() methods in the LangSmith SDK. It covers defining an application, creating or selecting a dataset, defining an evaluator, and running the evaluation, with code examples for both Python and TypeScript.
- [Log user feedback using the SDK](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/attach_user_feedback.md): This document explains how to log user feedback for traces using the LangSmith SDK. It covers using the create_feedback() and createFeedback() methods, attaching feedback to specific child runs, and non-blocking feedback creation in Python. Examples are provided for both Python and TypeScript.
- [Analyze Single Experiment](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/analyze_single_experiment.md): This guide explains how to use LangSmith's experiment view to analyze the results of a single experiment. It covers opening the experiment view, understanding the results table, utilizing heatmap and table views, sorting and filtering data, viewing traces and evaluator runs, grouping results by metadata, and comparing experiments.
- [Testing a ReAct agent with Pytest/Vitest and LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/tutorials/testing.md): This tutorial demonstrates how to use LangSmith's integrations with Pytest and Vitest/Jest to evaluate LLM applications. It guides users through creating a ReAct agent for stock-related queries and setting up a comprehensive test suite for it, utilizing tools like LangGraph, OpenAI, Tavily, E2B, and Polygon.
- [Evaluate a complex agent](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/tutorials/agents.md): This tutorial demonstrates how to build and evaluate a customer support bot using LangGraph and LangSmith. It covers three key evaluation types: final response, trajectory, and single step. The bot interacts with a sample music store database (Chinook) to handle customer requests for song lookups and refunds. The tutorial includes setup instructions, database download, and the definition of the agent's logic, including refund and lookup subgraphs, with a focus on extracting and routing customer and purchase information.
- [Langsmith. Expect](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/python/_expect/langsmith._expect._Expect): A summary for langsmith._expect._Expect.
- [Running SWE-bench with LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/tutorials/swe-benchmark.md): This document provides a comprehensive guide on integrating the SWE-bench benchmark with LangSmith for evaluating coding agents. It details the process of loading SWE-bench data, preprocessing the 'version' column to prevent data conversion errors, and uploading the dataset to LangSmith, either manually or programmatically via the SDK. The guide also explains how to run predictions using a dummy function and how to evaluate these predictions using Dockerized environments managed by SWE-bench's harness. Key steps include setting up the data schema in LangSmith, managing dataset splits for efficient testing, and converting Docker evaluation logs into a format compatible with LangSmith feedback.
- [Backtesting an Agent on LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/tutorials/backtesting.md): This document outlines the process of backtesting a new version of an agent using LangSmith. It details steps like selecting production runs, transforming inputs into a dataset, and comparing outputs of different agent versions. The guide also covers setting up the environment, defining the agent with tools, simulating production data, and converting production traces into a dataset for benchmarking. It emphasizes the importance of backtesting for continuous improvement post-deployment.
- [RAG Evaluation with LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/tutorials/rag.md): This tutorial demonstrates how to evaluate Retrieval Augmented Generation (RAG) applications using LangSmith. It covers creating test datasets, running the RAG application, and measuring performance with metrics like answer relevance, accuracy, and retrieval quality. The example involves a bot that answers questions about Lilian Weng's blog posts, with code examples provided in both Python and TypeScript for setup, indexing, retrieval, and generation.
- [evaluation](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/tutorials/evaluation.md): This document provides a comprehensive guide on setting up chatbot evaluations using LangSmith. It covers creating datasets, defining metrics like correctness and concision using LLM-as-a-judge and Python functions, and running evaluations. The guide aims to help users measure and iterate on their chatbot's performance with confidence by providing clear steps and code examples for setup, data creation, and evaluation metric implementation.
- [Concepts in LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/prompt_engineering/concepts.md): LangSmith focuses on prompt engineering, treating it as a core pillar for AI applications. It provides tools to facilitate prompt engineering, which guides model behavior similarly to how a director guides an actor. Prompt engineering allows customization of model responses and is often more accessible than fine-tuning, making it ideal for cross-disciplinary teams. The platform distinguishes between raw prompts and prompt templates, which include variables for customization. LangSmith supports both chat-style (list of messages) and completion-style (string) prompts, with f-string and mustache formatting options for variables. It also includes features for managing prompts, such as versioning, commits, tags, and a playground for testing and iteration.
- [Multiple Messages](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/prompt_engineering/how_to_guides/multiple_messages.md): A summary for multiple_messages.md.
- [Using Tools in Prompts with LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/prompt_engineering/how_to_guides/use_tools.md): This document explains how to use built-in and custom tools in LangSmith prompts. It covers adding tools, using built-in tools like web search and image generation from OpenAI and Anthropic, and creating custom tools with specified names, descriptions, and arguments. The document also touches upon tool choice settings for models and provides links to further documentation.
- [Trigger a webhook on prompt commit](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/prompt_engineering/how_to_guides/trigger_webhook.md): This document explains how to configure and trigger webhooks on prompt commits in LangSmith. It covers use cases such as CI/CD integration and prompt synchronization, and details the steps for setting up a webhook URL and headers. The document also describes the payload structure for test notifications and how to skip webhooks when using the API or Playground.
- [Including Multimodal Content in LangSmith Prompts](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/prompt_engineering/how_to_guides/multimodal_content.md): This document explains how to include multimodal content, such as images and PDFs, in prompts using LangSmith. It covers two methods: inline embedding of static files and the use of template variables for dynamic content. The guide also touches on model compatibility and running evaluations with multimodal inputs.
- [Prompt Canvas in LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/prompt_engineering/how_to_guides/prompt_canvas.md): The Prompt Canvas in LangSmith is a feature that allows users to easily edit prompts with the help of an LLM, enabling faster iteration and stylistic changes. It includes features like a chat sidebar for rewriting prompts, direct editing options, quick actions for adjusting reading level and length, custom quick actions, diffing to compare prompt versions, and the ability to save edited prompts.
- [Prompt Tags](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/prompt_engineering/how_to_guides/prompt_tags.md): A summary for prompt_tags.md.
- [Managing Model Configurations in LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/prompt_engineering/how_to_guides/managing_model_configurations.md): This document explains how to manage prompt settings in the LangSmith playground, including model configurations, tool settings, and prompt formatting. It details how to create, save, set as default, edit, and delete model configurations, and also covers tool settings, prompt formatting, and the use of extra parameters for unsupported model parameters or troubleshooting.
- [Update a prompt](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/prompt_engineering/how_to_guides/update_a_prompt.md): This document outlines how to update a prompt in LangSmith. It covers updating prompt metadata like descriptions and use cases, as well as editing the prompt content directly within the playground. The process of versioning prompts upon committing changes is also explained.
- [Custom OpenAI Compliant Model](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/prompt_engineering/how_to_guides/custom_openai_compliant_model.md): This document explains how to use the LangSmith playground with any OpenAI-compliant model provider or proxy. It covers deploying models using services like LiteLLM Proxy or Ollama and integrating them into the LangSmith Playground by setting the Proxy Provider for OpenAI.
- [custom_endpoint](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/prompt_engineering/how_to_guides/custom_endpoint.md): This document explains how to use your own custom models with the LangSmith playground through LangServe. It covers deploying a sample model server, adding configurable fields for parameters like temperature, and integrating the custom model into the LangSmith Playground by providing its URL.
- [Syncing Prompts with GitHub via LangSmith Webhooks](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/prompt_engineering/tutorials/prompt_commit.md): This document explains how to sync prompts from LangSmith with a GitHub repository using webhooks. It covers the benefits of version control and CI/CD integration, prerequisites like a GitHub account and PAT, and understanding LangSmith prompt commits. It also provides an overview of implementing a FastAPI server to receive webhook notifications and commit prompt manifests to GitHub.
- [Optimize a Classifier using LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/prompt_engineering/tutorials/optimize_classifier.md): This tutorial demonstrates how to optimize a classifier by incorporating user feedback. It covers building a GitHub issue classifier, collecting feedback for correct and incorrect predictions, and setting up automations to manage feedback loops. The process involves using LangSmith to trace runs, provide feedback, and create datasets for iterative model improvement, including an optional semantic search over examples.
- [LangSmith Evaluation Quick Start](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation.md): This guide explains how to perform quantitative performance evaluations for LLM applications using LangSmith. It covers creating datasets, defining target functions, and using evaluators, with examples provided for both SDK and UI users. The guide also details setting up dependencies, API keys, and environment variables, as well as creating datasets and defining the target function and evaluator using Python and TypeScript.
- [LangSmith Client SDK](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/js.md): This package contains the TypeScript client for interacting with the LangSmith platform. LangSmith helps you and your team develop and evaluate language models and intelligent agents, offering seamless integration with LangChain.js and compatibility with any LLM Application.
- [Observability Quick Start with LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability.md): This tutorial demonstrates how to integrate LangSmith's observability SDK to trace applications. It covers installation, API key setup, environment configuration, and tracing both OpenAI calls and entire applications using Python and TypeScript examples.
- [Run an evaluation with multimodal content](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/evaluate_with_attachments.md): This document explains how to use LangSmith to create dataset examples with file attachments, such as images, audio, and documents, for evaluating applications that handle multimodal inputs or outputs. It covers methods for uploading attachments using both SDKs (Python and TypeScript) and highlights the benefits of using attachments over base64 encoding, including faster transfer speeds and enhanced UI visualization. The document also details how to define a target function that can consume these attachments for evaluation purposes.
- [traceable](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/python/run_helpers/langsmith.run_helpers.traceable): The `traceable` function in Langsmith allows you to trace function calls, which automatically logs spans to LangSmith. It supports various configurations like specifying run type, name, metadata, tags, client, and custom processing functions for inputs and outputs.
- [wrapOpenAI](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/js/functions/wrappers_openai.wrapOpenAI): The wrapOpenAI function enhances OpenAI client instances by integrating LangSmith tracing capabilities. It allows for automatic tracing of OpenAI API calls, including completions, without altering the original method signatures. An optional `langsmithExtra` field can be passed within the parameters to include additional metadata for tracing, as demonstrated in the provided example.
- [wrap_openai._openai](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/python/wrappers/langsmith.wrappers._openai.wrap_openai): Patches the OpenAI client to make it traceable, supporting both Chat and Responses API, sync/async clients, and streaming. It allows for customization of run names for chat and completions endpoints.
- [LangSmith Modules v0.3.33](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/js/modules.md): This document lists and links to various modules available in LangSmith v0.3.33. It includes modules for anonymization, client, evaluation, testing utilities (Jest, Vitest), LangChain integration, run trees, schemas, traceable functions, and wrappers for services like Vercel and OpenAI.
- [LangSmith Run Helpers](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/python/_modules/langsmith/run_helpers.md): This file contains helper functions for creating and managing run trees in LangSmith, including decorators for tracing functions, managing tracing context, and inspecting function traceabilty. It supports asynchronous operations and integrates with LangSmith clients.
- [tracing_context](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/python/run_helpers/langsmith.run_helpers.tracing_context): This function sets the tracing context for a block of code, allowing for customization of project name, tags, metadata, and parent runs for logging purposes.
- [trace](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/python/run_helpers/langsmith.run_helpers.trace): Manages a LangSmith run in context. This class can be used as both a synchronous and asynchronous context manager, allowing for detailed tracking and management of operations within LangSmith.
- [LangSmithExtra](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/python/run_helpers/langsmith.run_helpers.LangSmithExtra): This class, LangSmithExtra, allows for dynamically injecting additional information into a runnable, such as an optional name, reference example ID, run information, parent run, run tree, project name, metadata, tags, run ID, and an optional LangSmith client or callback function to be invoked when the run ends.
- [RunTreeConfig](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/js/interfaces/run_trees.RunTreeConfig): Interface RunTreeConfig defines the configuration for a RunTree, including optional properties like attachments, child runs, client, metadata, and more, with a mandatory string name.
- [_openai](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/python/_modules/langsmith/wrappers/_openai.md): This file contains Python code for integrating with OpenAI, including functions to handle streaming responses, reduce choices, and infer invocation parameters for LangSmith tracing.
- [OpenAIAgentsTracingProcessor](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/python/wrappers/langsmith.wrappers._openai_agents.OpenAIAgentsTracingProcessor): A LangSmith tracing processor for the OpenAI Agents SDK that traces all intermediate steps of your OpenAI Agent to LangSmith.
- [vitest](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/js/modules/vitest.md): This document provides an overview of the vitest module in LangSmith, version v0.3.33. It details available functions such as describe, expect, it, and test, and lists various references including Jest-like wrappers and utility functions for logging feedback and outputs.
- [Utils Jestlike](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/js/modules/utils_jestlike.md): A summary for utils_jestlike.md.
- [schemas](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/js/modules/schemas.md): Provides TypeScript interfaces and type aliases for LangSmith, covering aspects like datasets, runs, feedback, and prompts, facilitating data management and interaction within the LangSmith ecosystem.
- [jest](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/js/modules/jest.md): This document outlines the Jest module in LangSmith v0.3.33, detailing its type aliases such as LangSmithJestDescribeWrapper and SimpleEvaluationResult, and functions like describe, expect, it, test, and wrapEvaluator. It also provides references to logging functions within the utils_jestlike module.
- [evaluation](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/js/modules/evaluation.md): This document outlines the evaluation module in LangSmith, version 0.3.33. It details various components including classes like StringEvaluator, interfaces such as EvaluateOptions and RunEvaluator, and type aliases like DataT and EvaluationResult. It also lists functions like evaluate and evaluateComparative, providing links to further documentation for each element.
- [client](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/js/modules/client.md): This document provides an overview of the 'client' module in LangSmith v0.3.33, detailing its classes, interfaces, type aliases, variables, and functions, with links to specific documentation for each element.
- [Get started with LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/.md): LangSmith is a platform for building and evaluating LLM applications, offering features for observability, evals, and prompt engineering. It integrates with LangChain and LangGraph for enhanced tracing and allows for detailed monitoring, performance analysis, and prompt iteration.
- [Get started with LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/.md): LangSmith is a platform for building and evaluating LLM applications, offering features for observability, evals, and prompt engineering. It integrates seamlessly with LangChain and LangGraph, providing tools to monitor, debug, and optimize LLM applications throughout the development lifecycle.
- [Prompt Engineering How-To Guides](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/prompt_engineering/how_to_guides.md): This document provides step-by-step guides for prompt engineering tasks in LangSmith, covering prompt organization, management, and iteration using the Playground, Prompt Hub, and Few-shot prompting techniques.
- [Create a prompt](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/prompt_engineering/how_to_guides/create_a_prompt.md): A guide to creating prompts in LangSmith, covering message roles (system, human, AI), template formats (f-string, mustache), inserting variables using curly braces or the UI, and using sample inputs for testing.
- [Compare Experiment Results in LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/compare_experiment_results.md): This document provides a guide on how to compare experiment results in LangSmith. It covers opening the comparison view, adjusting table displays, identifying regressions and improvements, filtering results, updating baseline experiments and metrics, opening traces, expanding detailed views, viewing summary charts, and using experiment metadata as chart labels.
- [Evaluation how-to guides](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides.md): This document provides a comprehensive guide to evaluating language model applications using LangSmith. It covers key features such as dataset management, running offline and online evaluations, analyzing results, and incorporating feedback. The guide is structured into sections addressing specific how-to questions, with links to detailed explanations and the API reference.
- [Dashboards LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/dashboards.md): LangSmith dashboards provide high-level insights into trace data, enabling users to monitor application health and spot trends. Users can leverage prebuilt dashboards, automatically generated for each project, or create custom dashboards with configurable charts tailored to specific needs. The platform offers features for visualizing metrics like trace counts, latency, token usage, and costs, with options to group and filter data for detailed analysis.
- [prebuilt_evaluators](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/evaluation/how_to_guides/prebuilt_evaluators.md): This document explains how to use prebuilt evaluators from the openevals package in LangSmith. It covers setup instructions for installing the package and configuring API keys, as well as demonstrating how to run evaluators with code examples in both Python and TypeScript. The guide also shows how to integrate evaluators with LangSmith's evaluate method.
- [Trace with LangChain (Python and JS/TS) | LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/observability/how_to_guides/trace_with_langchain.md): This document provides a guide on integrating LangSmith tracing with LangChain for both Python and JavaScript/TypeScript applications. It covers installation, environment configuration, logging traces, selective tracing via callbacks or context managers, and logging to specific projects either statically or dynamically.
- [Changelog | LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/changelog.md): This document details new features, enhancements, and bug fixes for the LangSmith application, organized by release date and version, including updates to usage charts, cost tracking, prompt synchronization, agent observability, and multimodal support.
- [LangChain Off-the-Shelf Evaluators](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/sdk_reference/langchain_evaluators.md): LangChain provides a module with various off-the-shelf evaluators for common evaluation scenarios, including Q&A, contextual Q&A, chain of thought Q&A, criteria-based evaluation, and string/JSON comparison. These evaluators, primarily available for Python, offer flexibility with customizable criteria and distance metrics but should be used as part of a holistic testing strategy.
- [Regions FAQ](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/regions_faq.md): This document provides answers to frequently asked questions about LangSmith regions, covering legal and compliance information such as data protection and GDPR, feature availability and differences between US and EU instances, and details on plans and pricing, confirming that pricing and core features are the same across regions, though the EU instance may have slight delays in feature launches.
- [Dataset Transformations in LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/evaluation/dataset_transformations.md): LangSmith enables dataset transformations to preprocess data before adding it to a dataset. It supports various transformation types like removing system messages, converting data to OpenAI format, and removing extra fields. The platform offers a prebuilt schema for chat models to simplify data collection and standardization for evaluations and few-shot prompting.
- [Authentication Methods for LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/authentication_authorization/authentication_methods.md): LangSmith offers various authentication methods, including email/password, social providers (GitHub, Google), and SAML SSO for enterprise users. Self-hosted instances support SSO via OAuth 2.0/OIDC, basic email/password authentication, and a "None" option for specific use cases.
- [Cloud Architecture and Scalability of LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/reference/cloud_architecture_and_scalability.md): LangSmith, a cloud-managed solution on GCP, is designed for high scalability, serving production workloads for LLM application observability and evaluation. It details its US and EU service architectures, regional and region-independent storage solutions including GCP services like GKE, GCS, Cloud SQL, and Memorystore, and lists IP addresses for whitelisting.
- [Self-Hosting LangSmith](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/langsmith/self_hosting.md): Step-by-step guides for installing, configuring, and scaling a self-hosted LangSmith instance, covering architecture, deployment options (Kubernetes, Docker), various configuration settings like SSO and database connections, usage, upgrades, and troubleshooting.
