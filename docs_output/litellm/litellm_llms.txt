# Document Index
- [LiteLLM Getting Started Guide](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs.md): This guide explains how to use LiteLLM, a library that allows calling over 100 LLMs using the OpenAI input/output format. It details two primary methods of usage: LiteLLM Proxy Server for a unified LLM gateway and LiteLLM Python SDK for direct integration into Python code. The document provides basic usage examples for various providers like OpenAI, Anthropic, and HuggingFace, and explains how to handle streaming responses.
- [LiteLLM Getting Started Guide](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs.md): This guide provides instructions on how to use LiteLLM, a library that allows you to call over 100 LLMs using the OpenAI input/output format. It covers both the LiteLLM Proxy Server and the LiteLLM Python SDK, detailing their use cases and basic operational examples. The document also explains the response format and how to use streaming functionality.
- [Reliability Features in LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/completion/reliable_completions.md): This document details how LiteM provides reliability for API requests through retries and fallbacks. It covers helper utilities, retry mechanisms, and various fallback strategies like context window adjustments and switching between models, API keys, or bases. The document also includes implementation details for these fallback features, such as cooldown periods for rate-limited models.
- [Docker Quick Start Guide for LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/docker_quick_start.md): This guide provides a comprehensive tutorial for using LiteLLM Proxy with Docker. It covers setting up Azure OpenAI models, making successful chat completion calls, generating virtual keys for access control, and implementing rate limiting. Prerequisites include installing the LiteLLM Docker image or CLI and having Docker installed. The tutorial details configuration via YAML files and demonstrates API calls using cURL.
- [Users](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/users.md): A summary for users.md.
- [Logging in LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/logging.md): This document details how to configure logging for LiteLLM, an open-source ngôn ngữ model orchestration tool. It covers enabling logging for proxy inputs, outputs, and exceptions through integrating with various services like Langfuse, OpenTelemetry, and cloud storage. The guide also explains how to generate and utilize a unique `call_id` for tracking requests, redact sensitive information such as message content and API keys, and dynamically disable specific logging callbacks. It further touches on conditional logging based on virtual keys and teams, and the standard logging payload structure.
- [Virtual Keys for LiteLLM Proxy](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/virtual_keys.md): This document provides a comprehensive guide to using virtual keys within the LiteLLM proxy for managing model access and tracking expenses. It covers setup requirements, including database configuration and master key setup, with detailed steps for generating keys via API calls. The guide also explains how to track spending per key, user, or team, and introduces model aliases for directing requests to different models, such as upgrading or downgrading user requests.
- [Custom Callbacks in LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/observability/custom_callback.md): This document explains how to implement custom callbacks in LiteLLM for logging pre-API calls, post-API calls, success events, and failure events, using both class-based and function-based approaches for synchronous and asynchronous operations.
- [Callbacks](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/observability/callbacks.md): Use liteLLM Callbacks to send output data to Posthog, Sentry, and other services. Supports custom functions, Lunary, Langfuse, LangSmith, Helicone, Traceloop, Athina, Sentry, PostHog, and Slack with a quick start guide provided.
- [Router - Load Balancing](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/routing.md): This document provides a comprehensive guide to using LiteLLM for load balancing across various LLM deployments. It covers essential features such as cross-deployment load balancing, request prioritization, and reliability logic including fallbacks and retries. The document details both SDK and Proxy configurations, offering code examples for setting up model lists and initializing the Router. It also explores advanced routing strategies like latency-based, least-busy, and cost-based routing, along with rate-limit-aware strategies utilizing Redis for usage tracking.
- [Output Format and Attributes in LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/completion/output.md): This document details the expected JSON output format and additional attributes, such as latency, for all LiteLLM completion calls across various models, providing examples for clarity.
- [Data Privacy and Security](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/data_security.md): This document outlines LiteLLM's commitment to data privacy and security for both LiteLLM Cloud and self-hosted instances. It details security measures such as data encryption, access controls, and data residency options, as well as security certifications including SOC 2 and ISO 27001. The document also covers data collection, cookie usage, vulnerability reporting, and vendor information, emphasizing the protection of user data.
- [Contributing Code to LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/extras/contributing_code.md): This document outlines the process for contributing code to the LiteLLM project. It details prerequisites like signing the Contributor License Agreement (CLA), adding tests, and ensuring code quality through unit and linting tests. It also provides instructions for setting up a local development environment, running tests, and submitting a Pull Request (PR). Additionally, it covers advanced topics such as building the LiteLLM Docker image.
- [Open WebUI with LiteLLM Integration](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/tutorials/openweb_ui.md): A guide on integrating Open WebUI with LiteLLM to access over 100 LLMs, track usage and spend, manage logs, and control model access. It covers starting LiteLLM and Open WebUI, creating virtual keys, connecting them, and testing requests.
- [Benchmarks](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/benchmarks.md): This document outlines the performance benchmarks for the LiteLLM Gateway, detailing its RPS and latency when tested against a fake OpenAI endpoint. It covers configurations for single and multiple LiteLLM instances, findings on how scaling instances affects performance, and the impact of logging callbacks like GCS Bucket and LangSmith on latency and RPS. The testing utilized specific machine specifications and Locust settings to simulate various load conditions.
- [Set Keys](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/set_keys.md): A summary for set_keys.md.
- [Exception Mapping for LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/exception_mapping.md): LiteLLM standardizes exceptions across various LLM providers, mapping them to OpenAI's exception types. It provides custom exception classes that inherit from OpenAI's, adding attributes like status_code, message, and llm_provider. The documentation details usage examples for error handling, including streaming exceptions and retry logic, and provides a comprehensive table mapping provider-specific exceptions to LiteLLM's standardized types.
- [Providers](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers.md): An extensive list of providers supported by LiteLLM, including major players like OpenAI, Azure, Anthropic, and AWS, as well as specialized services such as Cohere, Mistral AI, and more, with links for further details on each.
- [Enterprise](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/enterprise.md): This document outlines the enterprise features of LiteLLM, a service that provides solutions for companies needing SSO, user management, and professional support. It details both self-hosted and hosted options, including features, benefits, security, and support SLAs. The document also covers pricing and provides links for trials, demos, and further information.
- [Fallbacks in LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/reliability.md): This document explains how to implement fallback models in LiteLLM, a韓国 language model orchestration tool. It covers setting up fallbacks in the SDK and proxy, testing them, and controlling fallback prompts with examples for various integrations like OpenAI, Curl, and Langchain.
- [Mock Completion Responses with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/completion/mock_requests.md): This document explains how to use the `mock_response` feature in LiteLLM's `completion` function to simulate API responses for testing purposes without actually calling the LLM APIs. It covers quick start examples, streaming responses, the structure of mock response objects, and how to integrate this feature into pytest functions.
- [Setting Team Budgets in LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/team_budgets.md): This document outlines how to set and manage monthly team budgets within LiteLLM. It details the steps for creating teams, generating API keys associated with those teams, and testing budget limits through API calls. The guide also covers advanced features like Prometheus metrics for budget tracking and dynamic rate limiting for Temporal/Requests Per Minute (TPM/RPM) allocation, including priority-based quota reservation.
- [Billing with LiteLLM and Lago](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/billing.md): This document outlines how to set up and use Lago for usage-based billing with LiteLLM, covering internal team billing, customer billing, and internal user billing. It provides step-by-step instructions, code examples for cURL, Python, and Langchain, and details on configuring environment variables and API keys.
- [OpenMeter Usage-Based Billing for AI Applications](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/observability/openmeter.md): OpenMeter is an open-source solution for usage-based billing in AI and cloud applications, integrating seamlessly with Stripe. It allows developers to log LLM responses and track usage across providers with just two lines of code. The tool supports integration via SDK and Proxy, offering quick start guides for both.
- [Model Discovery](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/model_discovery.md): This document explains how to use the LiteLLM proxy to discover available models from various providers, including OpenAI, Gemini, and Vertex AI. It details the setup process using a config.yaml file and demonstrates how to call the /v1/models endpoint to retrieve a list of supported models.
- [Custom Prompt Management with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/custom_prompt_management.md): This document explains how to integrate a custom prompt management system with LiteLLM. It covers creating a custom prompt manager class, configuring it in LiteLLM's config.yaml, setting up the LiteLLM Gateway, and testing the integration with examples for OpenAI Python, Langchain, and cURL.
- [Alerting and Webhooks with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/alerting.md): This document outlines how to set up alerting and webhooks using LiteLLM. It covers configuration for various alert types such as LLM performance, budget tracking, and system health, with integrations for Slack, Discord, and Microsoft Teams. The guide includes a quick start for Slack alerts, advanced configurations for redacting messages, setting soft budget alerts for virtual keys, mapping alert types to specific channels, and adding metadata to alerts for debugging.
- [AWS Bedrock with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/bedrock.md): This document outlines how to use AWS Bedrock models with the LiteLLM library. It details supported models like Anthropic, Meta, Deepseek, and Amazon, along with LiteLLM routing and documentation. Instructions are provided for installation, authentication via boto3, and usage examples for both direct SDK calls and LiteLLM Proxy Server integration. The guide also covers setting parameters like temperature, top_p, and passing provider-specific arguments, including support for function calling.
- [Local LiteLLM Proxy Server](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy_server.md): A fast, lightweight OpenAI-compatible proxy server for calling 100+ LLM APIs, with usage examples for various models like Ollama, VLLM, Huggingface, and cloud providers. It also includes tutorials for integration with tools like LibreChat, AutoGen, and ChatDev.
- [Region-based Routing with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/customer_routing.md): This document explains how to configure LiteLLM for region-based routing, allowing specific customers to be directed to models within a designated geographical region (e.g., "eu"). It details the steps for creating a customer with region specifications using the "end-user" object, adding region-tagged models to a model group, and testing the configuration to ensure correct routing. This feature helps partition model access based on regional factors.
- [Multi-Instance TPM/RPM Load Test](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/load_test_rpm.md): This document outlines how to test throughput limits (TPM/RPM) across multiple instances of the liteLLM Router. It details a load test scenario sending 600 requests per minute to two router instances, each with a 100 RPM limit, expecting only 200 requests per minute to succeed. The guide also provides code examples for setting up a fake OpenAI server and running the load test script.
- [Instructor LiteLLM Integration](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/tutorials/instructor.md): This document explains how to integrate LiteLLM with the Instructor library for robust structured outputs. It covers both synchronous and asynchronous usage, demonstrating how to validate outputs into Pydantic types and handle validation errors for improved model responses.
- [Using ChatLiteLLM with Langchain](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/langchain.md): This document provides a guide on using ChatLiteLLM from Langchain, covering prerequisites, quick start examples for various models (OpenAI, Anthropic, Replicate, Cohere), and integration with observability tools like MLflow and Lunary. It also briefly mentions Langfuse integration.
- [Nebius AI Studio Integration with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/nebius.md): This document provides a comprehensive guide to integrating Nebius AI Studio models with LiteLLM. It covers setting up API keys, sample usage for text generation, streaming, and embeddings, and details on using the LiteLLM Proxy Server. The guide also outlines supported parameters for chat completion and embedding, along with common error handling for the integration.
- [Featherless AI Integration with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/featherless_ai.md): This document provides a guide on integrating Featherless AI models with LiteLLM, including setup instructions, API key configuration, and sample usage for both standard and streaming requests. It also lists supported chat models and their respective function call formats.
- [Snowflake](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/snowflake.md): This document provides a comprehensive guide to using the Snowflake Cortex LLM REST API with LiteLLM. It details how to access the COMPLETE function via HTTP POST requests, including the provider route, documentation link, and base URL. The guide also covers supported OpenAI parameters, API key authentication using JWT tokens and account identifiers, and provides usage examples for both direct LiteLLM calls and integration with the LiteLLM Proxy. Instructions are included for setting environment variables, configuring the proxy, and testing the integration.
- [Custom Llm Server](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/custom_llm_server.md): A summary for custom_llm_server.md.
- [SambaNova Integration with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/sambanova.md): This document provides a comprehensive guide to integrating SambaNova models with LiteLLM, covering API key setup, sample usage for both standard and streaming requests, and configurations for the LiteLLM proxy server. It also details how to leverage SambaNova models for tool calling and structured output, including a vision example.
- [OpenRouter](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/openrouter.md): This document outlines how to use the LiteLLM library with OpenRouter, supporting all their chat, text, and vision models. It details the setup process with API keys and provides examples for calling various OpenRouter models like those from Google, OpenAI, Anthropic, and Meta-LLaMA. The guide also explains how to pass specific OpenRouter parameters such as transforms, models, and routes during completion calls.
- [Prometheus Metrics in LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/prometheus.md): This document details how to enable and use Prometheus metrics within the LiteLLM Enterprise environment. It covers the Quick Start guide for setting up the `/metrics` endpoint, tracking various metrics related to virtual keys, teams, and proxy-level usage, as well as detailed LLM provider metrics for error monitoring and rate limit tracking.
- [Custom LLM Pricing](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/custom_pricing.md): This document provides a guide on how to set custom pricing for different language models using LiteLLM. It details two primary methods: cost per token and cost per second, offering step-by-step instructions for configuration and debugging. Examples are provided for SageMaker and Azure models, along with guidance on overriding default pricing and setting base models for accurate cost tracking.
- [Response Headers](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/response_headers.md): A summary for response_headers.md.
- [Prompt Management with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/prompt_management.md): This document explains how to manage prompts using LiteLLM integrations with tools like Langfuse and Humanloop. It covers setting up configurations, making API calls with prompt variables, and managing model configurations within LiteLLM or the integrated tools.
- [Secret Detection/Redaction](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/guardrails/secret_detection.md): Provides instructions on how to use LiteLLM's enterprise-only secret detection and redaction feature to automatically detect and redact sensitive information like API keys from LLM requests. It covers configuration in `config.yaml`, server setup with `litellm --config`, and testing with cURL commands, including how to customize which secret detectors are used.
- [Bedrock (boto3) SDK with liteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/pass_through/bedrock.md): This document provides a comprehensive guide to using the Bedrock (boto3) SDK with liteLLM, covering setup, proxy calls, and advanced use cases like virtual keys and agents. It details how to replace Bedrock endpoints with liteLLM proxy URLs and manage authentication, enabling seamless integration and cost-effective model usage.
- [Bedrock Knowledge Bases with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/bedrock_vector_store.md): AWS Bedrock Knowledge Bases allows you to connect your LLM's to your organization's data, letting your models retrieve and reference information specific to your business. This guide details how to integrate Bedrock Knowledge Bases with LiteLLM using both the Python SDK and the LiteLLM Proxy, including code examples for configuration and making requests.
- [Bedrock Agents with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/bedrock_agents.md): This document explains how to use Bedrock Agents with LiteLLM, covering both direct SDK usage and integration through the LiteLLM Proxy. It details the model format required, provides Python code examples for basic and streaming requests, and outlines the configuration steps for the LiteLLM Proxy with example cURL and Python SDK requests. The document also includes links to further reading on AWS Bedrock Agents and LiteLLM authentication.
- [Pagerduty](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/pagerduty.md): A summary for pagerduty.md.
- [Quick Start Guide for LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/quick_start.md): This guide explains how to use the LiteLLM CLI and configuration for managing LLM requests. It covers setting up the LiteLLM proxy, its features like unified interface, cost tracking, and load balancing, and provides examples for various LLM providers including OpenAI, Azure, Bedrock, and Huggingface.
- [Create your first LLM playground](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/tutorials/first_playground.md): This guide demonstrates how to create an LLM playground in under 10 minutes by setting up a backend server with Flask and connecting it to a Streamlit frontend. It covers environment setup, API key integration for multiple LLM providers like OpenAI, Cohere, and AI21, and testing the server with curl commands. The guide aims to enable users to evaluate various LLM providers efficiently.
- [Using Fine-Tuned GPT-3.5-Turbo with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/tutorials/finetuned_chat_gpt.md): This document explains how to use LiteLLM to call fine-tuned GPT-3.5-Turbo models, including setting up API keys and organization IDs, providing code examples for both scenarios.
- [Llama2 Together AI Tutorial | liteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/tutorials/TogetherAI_liteLLM.md): This document provides a comprehensive tutorial on using Llama2 models with Together AI via the liteLLM library. It covers basic API calls, streaming responses, and advanced usage with custom prompt templates for different Llama2 variants. The tutorial includes code examples and explanations for setting up API keys and formatting messages.
- [Llama2 - Huggingface Tutorial](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/tutorials/huggingface_tutorial.md): This tutorial explains how to use the litellm library to call Llama2 models with Huggingface Inference Endpoints. It covers connecting to default, public, and private Huggingface endpoints, including how to handle authentication for private endpoints using environment variables, package variables, or direct API key passing.
- [huggingface_codellama](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/tutorials/huggingface_codellama.md): This tutorial demonstrates how to utilize CodeLlama models hosted on Huggingface Inference Endpoints for code infilling tasks. It outlines the prerequisites, usage with LiteLLM, and provides an example of generating code to fill a masked section, focusing on the 7B and 13B variants suitable for this specialized capability.
- [Gradio Chatbot + LiteLLM Tutorial](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/tutorials/gradio_integration.md): A simple tutorial demonstrating how to integrate LiteLLM completion calls with streaming Gradio chatbot demos. It covers dependency installation, defining an inference function, setting up the chat interface, and launching the Gradio app, with recommendations for extensions.
- [Azure OpenAI Completion](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/tutorials/azure_openai.md): This document provides a guide on how to use the Completion() API with Azure OpenAI, covering quick start, streaming, async streaming, and multi-threaded calls. It includes code examples for both OpenAI and Azure OpenAI configurations.
- [Maximum Retention Period for Spend Logs](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/spend_logs_deletion.md): This document explains how to configure the maximum retention period for spend logs in LiteLLM Enterprise, which helps manage database size by automatically deleting old logs. It covers setup, configuration options for retention period and cleanup interval, and the underlying process including optional Redis-based locking and batch deletion. Requirements include Postgres and optionally Redis for distributed locking.
- [JWT Auth Architecture in LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/jwt_auth_arch.md): This document outlines how to implement JWT authentication in LiteLLM Enterprise, detailing configuration for Azure AD and Keycloak. It explains how to map JWT roles to LiteLLM roles and set up role-based access control (RBAC) for model access. The document covers example JWT structures, proxy configuration settings like enable_jwt_auth, user_roles_jwt_field, user_allowed_roles, enforce_rbac, and role_permissions, along with a step-by-step guide on how the JWT authentication process works within LiteLLM.
- [Database Information for LiteLLM Proxy](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/db_info.md): The LiteLLM Proxy utilizes a PostgreSQL database to store information including virtual keys, organizations, teams, users, budgets, and tracks per-request usage. It details various tables for managing these entities, authentication, model configurations, budget constraints, and includes tracking and logging functionalities for API requests and system changes. The document also explains how to disable certain logging features and outlines essential tables for database migration.
- [DB Deadlocks with High Availability Setup](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/db_deadlocks.md): This document explains how to resolve database deadlocks in a high-traffic environment by implementing a high availability setup in LiteLLM. It details how using a Redis queue and a single instance for database writes prevents deadlocks caused by multiple LiteLLM instances simultaneously updating the same data. Configuration and monitoring metrics are also covered.
- [Clientside LLM Credentials](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/clientside_auth.md): This document explains how to configure clientside authentication for LLM API keys and provider-specific parameters using LiteLLM. It covers passing user API keys, enabling configurable credentials for providers like Fireworks AI, and passing parameters such as region and project ID for services like Vertex AI.
- [Jina AI Integration with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/jina_ai.md): This document provides instructions and code samples for integrating Jina AI with LiteLLM for embeddings and reranking. It covers API key setup, SDK and proxy usage for both embeddings and reranking, supported models, and provider-specific parameters.
- [voyage](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/voyage.md): This document provides a guide to using Voyage AI embeddings with the LiteLLM. It covers API key setup, sample code for generating embeddings with various Voyage models, and a list of supported Voyage models.
- [Novita AI Integration with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/novita.md): This document provides a comprehensive guide to integrating Novita AI with LiteLLM. It covers setting up API keys, understanding supported parameters, and demonstrates sample usage for various features like SDK, Proxy, Tool Calling, and JSON Mode. It also lists all Novita AI chat models supported by LiteLLM.
- [togetherai](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/togetherai.md): This document provides a guide on how to use the Together AI platform with liteLLM. It covers API key setup, sample usage for various models including Llama, Falcon, Alpaca, and others, and details on prompt templates for chat and language models. The guide also includes code examples for integrating with different models and setting up custom prompt templates.
- [Replicate](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/replicate.md): A summary for replicate.md.
- [NLP Cloud Integration with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/nlp_cloud.md): This document details how to integrate with NLP Cloud using the LiteLLM library. It covers setting up API keys, provides sample usage for text generation and streaming, and explains how to use non-Dolphin models like Llama-2 by specifying custom model paths.
- [AI21 Models in LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/ai21.md): This document outlines how to use AI21 models with LiteLLM, covering supported models, API key setup, and usage examples for both the Python SDK and proxy server. It details how to pass specific AI21 parameters like "documents" and lists supported parameters for seamless integration.
- [DeepInfra Documentation for LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/deepinfra.md): This document provides instructions on how to use DeepInfra models with LiteLLM. It details how to set up your API key and includes sample code for both standard and streaming API requests. The document also lists various supported chat models available through DeepInfra.
- [Cloudflare Workers AI with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/cloudflare_workers.md): This document provides instructions and code samples for using Cloudflare Workers AI with the LiteLLM library. It details API key setup, demonstrates basic and streaming usage with various models like Llama-2 and CodeLlama, and lists supported models with their corresponding LiteLLM function calls.
- [Xinference](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/xinference.md): This document provides instructions on how to use Xinference with the LiteLLM library. It covers setting up the API base and key, provides sample code for generating embeddings, and lists all supported models.
- [Llamafile](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/llamafile.md): LiteLLM documentation for Llamafile, detailing how to use it with LiteLLM for seamless LLM distribution and execution via a single file. It covers setup for both direct completion calls and the LiteLLM Proxy Server, including examples for chat completions and embeddings.
- [Infinity | liteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/infinity.md): This document provides a comprehensive guide to using Infinity, a high-throughput, low-latency REST API for serving text-embeddings, reranking models, and clip, with the liteLLM Python SDK and Proxy. It includes setup instructions, usage examples for reranking and embeddings, and details on passing provider-specific parameters.
- [VLLM LiteLLM Integration Guide](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/vllm.md): This document provides a comprehensive guide to integrating VLLM with LiteLLM for efficient LLM inference and serving. It covers setup, usage examples for completion and embeddings via both SDK and proxy server, and specifics on handling video content.
- [Clarifai integration with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/clarifai.md): This document provides a guide on integrating Clarifai models with LiteLLM, supporting various LLMs like Anthropic, OpenAI, Mistral, Llama, and Gemini. It covers prerequisites, environment variable setup, and provides code examples for using different Clarifai models. Note that streaming is not yet supported.
- [Deepseek LiteLLM Integration](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/deepseek.md): This document provides a comprehensive guide to integrating Deepseek models with LiteLLM. It covers API key setup, sample usage for both standard and streaming requests, and details supported chat and reasoning models. Instructions are also included for setting up and testing the LiteLLM proxy with Deepseek models.
- [Fireworks AI Integration with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/fireworks_ai.md): This document provides a comprehensive guide on integrating LiteLLM with Fireworks AI, covering serverless models, custom account models, and direct-route deployments. It includes instructions for API key setup, sample usage with code snippets for both serverless and streaming models, and configurations for LiteLLM Proxy with various testing methods like curl, OpenAI client, and Langchain. The guide also details document inlining capabilities for non-vision models through URL transformations.
- [Groq LiteLLM Integration](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/groq.md): This document provides a comprehensive guide to integrating Groq models with LiteLLM. It details how to set up API keys, use Groq models in sample usage scenarios including streaming, and configure them with the LiteLLM Proxy. The guide also covers supported Groq models and provides examples for tool/function calling and vision capabilities.
- [Github](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/github.md): This document provides instructions on how to use Github models with the liteLLM library. It covers API key setup, sample usage for both standard and streaming requests, and configuration for the liteLLM Proxy. The guide also includes examples for tool/function calling with Github models.
- [Galadriel LiteLLM Integration](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/galadriel.md): This document provides instructions on how to integrate LiteLLM with Galadriel, a platform supporting various language models. It covers API key setup, sample code for direct and streaming requests, and a list of supported models with their simplified names and function call examples.
- [FriendliAI Integration with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/friendliai.md): This document outlines how to integrate FriendliAI models with LiteLLM, supporting all FriendliAI models by prefixing with "friendliai/". It details setup, API key configuration, and provides sample code for both standard and streaming API requests, including supported models like meta-llama-3.1.
- [Perplexity AI (pplx-api) with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/perplexity.md): This document provides a guide on integrating Perplexity AI with LiteLLM, covering API key setup, sample usage for both standard and streaming requests, and advanced features like reasoning effort configuration. It also lists the supported Perplexity AI models accessible through LiteLLM and offers examples for using the LiteLLM proxy.
- [Ollama LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/ollama.md): This document provides a comprehensive guide on integrating Ollama with LiteLLM, covering setup, example usage for chat and completions, streaming, JSON mode, and tool calling. It also details how to use the LiteLLM proxy with Ollama and lists supported Ollama models compatible with LiteLLM.
- [Triton Inference Server Integration with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/triton-inference-server.md): This document outlines how to integrate Triton Inference Server with LiteLLM, detailing support for embedding models and providing code examples for various operations including chat completion and embeddings via both SDK and proxy. It covers configuration steps for models and proxy startup, along with request examples using OpenAI Python client and curl.
- [Volcano Engine (Volcengine) | liteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/volcano.md): This document provides instructions on how to use Volcano Engine's NIM models with the liteLLM library. It covers API key setup, sample usage for both standard and streaming requests, and configuration for the liteLLM Proxy. The library supports all Volcengine NIM models by setting a specific model prefix in requests.
- [Cerebras LiteLLM Integration](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/cerebras.md): This document provides instructions on how to integrate Cerebras models with LiteLLM. It covers setting up the API key, using sample code for direct completion and streaming, and configuring the LiteLLM proxy server for Cerebras models. Ensure the model name is prefixed with "cerebras/" to route requests correctly.
- [LM Studio Integration with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/lm_studio.md): This document provides a comprehensive guide on integrating LM Studio with LiteLLM. It details how to discover, download, and run local LLMs using LM Studio, with specific instructions on configuring LiteLLM to use LM Studio models. The guide covers setting up API keys, using sample code for various functionalities like streaming and embeddings, and integrating with the LiteLLM Proxy Server. It also outlines supported parameters and features like structured output via JSON Schema.
- [xAI integration with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/xai.md): This document provides a comprehensive guide on integrating xAI models with LiteLLM. It covers setting up API keys, utilizing sample code for basic, streaming, and vision functionalities, and configuring the LiteLLM proxy server. Additionally, it explains how to leverage reasoning capabilities with various xAI models by adjusting the reasoning_effort parameter.
- [Nscale (EU Sovereign) | LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/nscale.md): This document provides a guide on integrating with Nscale, a European-domiciled AI cloud platform, using the LiteLLM SDK and proxy. It details how to use Nscale models for text and image generation, including setup, available models, and code examples for both streaming and non-streaming responses. The guide also covers configuration for the LiteLLM Proxy and highlights Nscale's key features such as EU sovereignty, low cost, and production-grade reliability.
- [Nvidia NIM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/nvidia_nim.md): Nvidia NIM is a platform that allows users to deploy and use AI models via a simple API. LiteLLM provides support for all models available on Nvidia NIM, enabling users to specify models using the `nvidia_nim/<model_name>` format. The platform supports various OpenAI endpoints and requires an API key for authentication.
- [Predibase Integration with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/predibase.md): This document outlines how to integrate Predibase models with LiteLLM for easy deployment and usage. It covers setting up API keys, making example calls using the SDK and proxy, advanced prompt formatting, and passing additional or model-specific parameters like max_tokens, temperature, adapter_id, and adapter_source. The guide provides code snippets for both SDK and proxy usage, demonstrating various configurations and functionalities for seamless integration.
- [IBM watsonx.ai with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/watsonx.md): This document provides a guide on integrating IBM watsonx.ai models with LiteLLM for various applications, including text generation, chat, and embeddings. It details environment variable setup, usage examples for both direct API calls and streaming, and explanations for using models within deployment spaces. The guide also covers OpenAI proxy usage and authentication methods, and lists supported watsonx.ai models.
- [Deepgram LiteLLM Integration](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/deepgram.md): LiteLLM supports Deepgram's /listen endpoint, enabling speech-to-text, text-to-speech, and language understanding. The integration uses the `deepgram/` provider route and supports OpenAI's `/audio/transcriptions` endpoint. Documentation includes quick start examples and proxy usage with configuration and testing steps.
- [Databricks LiteLLM Integration](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/databricks.md): This document provides a comprehensive guide on integrating LiteM with Databricks, detailing how to leverage Databricks models through LiteM's SDK and proxy. It covers environment variable setup, example API calls for various Databricks models like DBRX Instruct and Llama-3, and configuration details for the LiteM proxy. Additionally, it explains how to pass additional parameters such as max_tokens and temperature, and how LiteM translates reasoning effort to Anthropic's thinking parameter for models like Claude 3.
- [Hugging Face](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/huggingface.md): This document provides a guide to using Hugging Face with LiteLLM. It covers serverless inference providers like Together AI and Sambanova, as well as dedicated Inference Endpoints. The guide also explains authentication, usage formats, and provides examples for basic completion, streaming, image input, and function calling.
- [Anyscale](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/anyscale.md): This document provides a guide on using Anyscale with liteLLM, covering API key setup, sample usage for both standard and streaming requests, and a list of supported models with their respective function call formats.
- [Cohere LiteLLM Integration](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/cohere.md): This document provides a comprehensive guide to integrating Cohere models with LiteLLM, covering API key setup, Python SDK usage for text generation and embeddings, and instructions for using the LiteLLM Proxy. It details supported models, including text generation, embedding, and reranking capabilities, with code examples for seamless implementation.
- [Codestral API with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/codestral.md): This document provides a comprehensive guide to using the Codestral API, a powerful tool for code generation and completion, through the LiteLLM library. It details how to set up your API key, utilize both non-streaming and streaming text completion endpoints with sample code, and outlines the supported Codestral models. Additionally, it covers the chat completion functionality, including non-streaming and streaming examples, and lists the specific models available for chat interactions.
- [Mistral AI API with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/mistral.md): This document provides a guide on how to use the Mistral AI API with LiteLLM, covering API key setup, sample usage, streaming, and integration with the LiteLLM proxy. It details supported models, function calling capabilities, and reasoning features, including how to configure and utilize the `reasoning_effort` parameter for specific models like Magistral.
- [Meta Llama](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/meta_llama.md): Meta's Llama API, accessible via LiteLLM, provides access to Meta's family of large language models. It supports various endpoints including chat completions and standard completions, with specific models like Llama-4-Scout-17B-16E-Instruct-FP8 and Llama-3.3-70B-Instruct. The API can be used through the LiteLLM Python SDK with non-streaming and streaming capabilities, supporting function calling and tool use. Additionally, it can be integrated with the LiteLLM Proxy for streamlined access.
- [LiteLLM Proxy (LLM Gateway)](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/litellm_proxy.md): LiteLLM Proxy is an OpenAI-compatible gateway that unifies interaction with multiple LLM providers via a single API. It supports various endpoints including chat, completions, embeddings, image generation, and audio processing. The proxy can be set up using environment variables for API keys and base URLs, and it seamlessly integrates with popular libraries like Langchain and LlamaIndex.
- [AWS Sagemaker Integration with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/aws_sagemaker.md): This document explains how to integrate AWS SageMaker models with LiteLLM. It covers API key setup, usage examples for both standard and streaming requests, LiteLLM Proxy configuration, and advanced parameter settings like temperature and provider-specific parameters. It also details how to enable zero-temperature settings for SageMaker models.
- [Anthropic Models in LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/anthropic.md): This document outlines Anthropic's Claude models supported by LiteLLM, detailing their function calls, provider routes, API endpoints, and supported parameters. It also includes usage examples for direct API calls, streaming, and the LiteLLM Proxy, along with notes on environment variables and prompt caching.
- [Gemini - Google AI Studio Integration with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/gemini.md): This document details the integration of Google AI Studio's Gemini models with the LiteLLM library. It covers API key setup, sample usage for text generation and text-to-speech, supported parameters, and proxy configurations. The integration aims to simplify the use of Gemini models through a familiar OpenAI-like interface.
- [VertexAI LiteLLM Integration](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/vertex.md): This document provides a comprehensive guide to integrating Vertex AI with LiteLLM, covering its features, API routes, and practical examples for various operations like chat completions, function calling, and JSON schema validation. It also details how to configure and use the Vertex AI provider within LiteLLM, including authentication methods and proxy setup.
- [AI/ML API](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/aiml.md): This document provides a guide to integrating with the AI/ML API, covering how to obtain an API key, explore available models, and utilize the API for various tasks including text generation, streaming, and asynchronous operations. It also includes examples for embeddings and image generation.
- [Azure AI Studio litellm Integration](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/azure_ai.md): This document outlines how to integrate Azure AI Studio models with the LiteLLM library. It covers setup, usage examples for both SDK and proxy, including environment variable configuration, sample API calls, and details on passing parameters like max_tokens and temperature. The guide also explains function calling capabilities and lists supported Azure AI models, with a specific section on the Rerank endpoint.
- [Azure OpenAI with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/azure.md): This document provides a guide on integrating Azure OpenAI models with LiteLLM. It covers API key setup, usage examples for LiteLLM's Python SDK and proxy server, and lists supported Azure OpenAI models for chat and vision functionalities.
- [OpenAI-Compatible Endpoints with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/openai_compatible.md): This document outlines how to use LiteLLM to connect with OpenAI-compatible endpoints, including setup, request routing, and proxy server integration, with examples for completion, embedding, and advanced configurations like disabling system messages.
- [OpenAI Text Completion with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/text_completion_openai.md): This document provides a comprehensive guide on how to use OpenAI's text completion models with the LiteLLM library. It covers setting up API keys, basic usage examples, and advanced integration with the LiteLLM Proxy Server for streamlined deployment. The guide also includes details on specific OpenAI text completion models and their function calls.
- [OpenAI with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/openai.md): This document provides a guide on how to use OpenAI models with LiteLLM, covering API key setup, direct usage, and integration with the LiteLLM Proxy Server. It details various OpenAI models supported, including chat and vision models, and provides code examples for different use cases like direct API calls, proxy server testing with cURL, and integration with Langchain.
- [Caching with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/caching/all_caches.md): This document provides a comprehensive guide to implementing caching mechanisms within the LiteLLM library. It covers various caching strategies including in-memory, Redis, S3 buckets, Redis semantic cache, and disk cache. The guide includes installation instructions, code examples for initializing and using each cache type, and explanations of cache control parameters like no-cache, no-store, ttl, and s-maxage.
- [Budget Manager for LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/budget_manager.md): The Budget Manager in LiteLLM is a tool designed to help developers prevent unexpected bills from LLM API usage. It offers functionalities like setting a global maximum budget, managing per-user budgets, and integrating with a LiteLLM Proxy Server for advanced features लाइक user key management and spend tracking. The tool provides classes and methods to create, update, and query budget information, supporting different budget durations and offering both.

- [HuggingFace Rerank](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/huggingface_rerank.md): HuggingFace Rerank allows you to use reranking models hosted on Hugging Face infrastructure or your custom endpoints to reorder documents based on their relevance to a query. It supports the LiteLLM Python SDK, asynchronous requests, and integration with the LiteLLM Proxy for managing multiple reranking models and custom endpoints.
- [Custom Pricing for LLM APIs](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/sdk_custom_pricing.md): This document provides a guide on how to register custom pricing forvarious language models, including those from SageMaker and Azure. It explains how to set costs per second for models like Llama 2 on SageMaker and costs per token for Azure models, offering code examples for implementation.
- [Completion Token Usage & Cost with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/completion/token_usage.md): This document explains how LiteLLM provides token usage and cost information for completion requests. It details helper functions for encoding, decoding, counting tokens, and calculating costs. The document also covers managing model pricing and tokenizer configurations for various LLM models.
- [LiteLLM SDK Load Testing](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/load_test_sdk.md): This script provides a comparison of load testing the LiteLLM SDK against OpenAI. It includes code for setting up asynchronous clients for both OpenAI and LiteLLM, defining model lists for routing, and asynchronous functions to handle completions with and without streaming. The script demonstrates how to perform a load test by sending multiple concurrent requests and measuring the performance.
- [LiteLLM Proxy Load Testing Guide](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/load_test_advanced.md): This document provides a comprehensive tutorial on achieving 1K+ Requests Per Second (RPS) with LiteLLM Proxy using Locust. It covers pre-testing requirements, configuration for fake OpenAI endpoints and rate-limited endpoints, expected performance metrics, and debugging with Prometheus. The guide also details necessary machine specifications and provides a sample Locust file for load testing.
- [Google ADK with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/tutorials/google_adk.md): This tutorial demonstrates how to integrate Google's Agent Development Kit (ADK) with LiteLLM to build intelligent agents capable of interacting with multiple Large Language Model (LLM) providers, including OpenAI, Anthropic, and Google Gemini. It covers setting up the environment, defining tools, and creating agents that can switch between models for weather information retrieval.
- [Gemini Realtime API with Audio Input/Output using LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/tutorials/gemini_realtime_with_audio.md): This guide explains how to use the Gemini Realtime API for audio input and output with LiteLLM Proxy. It covers setting up the LiteLLM Proxy with a config.yaml file, starting the proxy, and running a Python test script that streams audio data and receives audio responses.
- [Aporia Guardrails with LiteLLM Gateway](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/tutorials/litellm_proxy_aporia.md): This tutorial demonstrates how to integrate Aporia with LiteLLM Gateway to implement guardrails for detecting PII in requests and profanity in responses. It covers setting up Aporia projects for pre- and post-LLM call policies, configuring LiteLLM with guardrail settings, and testing requests to ensure proper functionality.
- [Gemini Realtime API](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/google_ai_studio/realtime.md): This document outlines the usage of the Gemini Realtime API via the liteLLM proxy. It covers proxy setup, usage examples with Node.js, and lists limitations such as lack of audio transcription and tool calling support. The document also details supported OpenAI realtime events and session parameters.
- [Vertex AI Image Generation](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/vertex_image.md): Vertex AI Image Generation leverages Google's Imagen models to produce high-quality images from textual descriptions. The document provides a quick start guide for using the LiteLLM Python SDK and Proxy, including code examples for basic image generation. It also details how to configure and start the LiteLLM Proxy server and make requests using the OpenAI Python SDK. Information on supported models is available, with a note that all Vertex AI Image Generation models are supported by prefixing with `vertex_ai/`. A link to the LiteLLM models page is provided for further details.
- [Google AI Studio SDK with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/pass_through/google_ai_studio.md): This document explains how to use the Google AI Studio SDK with LiteLLM, including setting up pass-through endpoints, tracking costs and logging, and examples for various use cases like token counting and content generation. It also covers advanced configurations such as using virtual keys and sending tags in request headers. Supports all Google AI Studio endpoints, including streaming.
- [Google AI Studio (Gemini) Files API](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/google_ai_studio/files.md): This document provides a guide on how to use the Google AI Studio Files API with liteLLM. It explains how to upload files for use with the Gemini API, specifically its /generateContent endpoint. The guide includes setup instructions for both SDK and proxy usage, with code examples for creating and using files in Gemini's generative model.
- [Azure OpenAI Embeddings with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/azure/azure_embedding.md): This document provides a guide on how to use Azure OpenAI Embeddings with LiteLLM, covering API key setup, direct usage with code examples, and integration with the LiteLLM Proxy Server for streamlined API calls. It details the necessary parameters and demonstrates testing the integration through cURL requests and OpenAI client usage.
- [OpenAI - Response API](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/openai/responses_api.md): This document provides a comprehensive guide to using the Response API with LiteLLM, covering both the Python SDK and LiteLLM Proxy integrations. It details how to generate non-streaming and streaming responses, retrieve and delete responses by ID, and utilize reusable prompt templates. The guide also includes examples for computer use cases with specific tool parameters and configuration setups.
- [text_to_speech](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/openai/text_to_speech.md): This document provides instructions on using the LiteLLM SDK and Proxy for text-to-speech functionality. It includes code examples for both synchronous and asynchronous Python SDK usage, as well as a cURL command for the LiteLLM Proxy. The document also lists supported models and demonstrates how to configure the LiteLLM Proxy to set maximum request file sizes for audio transcription.
- [Azure Responses API guide](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/azure/azure_responses.md): This document provides a comprehensive guide to using the Azure Responses API with LiteLLM. It covers setup, non-streaming and streaming response examples using both the LiteLLM SDK and the OpenAI SDK with LiteLLM Proxy. The guide also includes specific instructions for integrating Azure Codex Models and utilizing the `/chat/completions` endpoint, detailing necessary configurations and code snippets for each scenario.
- [Routing based on request metadata](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/tutorials/tag_management.md): This document provides a guide on how to implement tag-based routing for LLM requests. It details the setup process, including enabling tag filtering in the configuration and creating custom tags. Instructions are provided on how to test these rules with examples of both invalid and valid model requests, demonstrating how tags control model access.
- [Prompt Caching Checkpoints with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/tutorials/prompt_caching.md): LiteLLM enables automatic prompt caching to reduce costs by up to 90% by injecting caching directives into requests. This feature works without modifying application code and can be configured via the LiteLLM UI or config.yaml file by specifying injection points for caching based on message role and location. The system caches long, static parts of prompts, processing them only once.
- [Msft Sso](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/tutorials/msft_sso.md): A summary for msft_sso.md.
- [Onboard Users for AI Exploration with Default Teams](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/tutorials/default_team_self_serve.md): This document outlines the process of onboarding users for AI exploration using LiteLLM. It details how to create a team with specified models and budget, update team member permissions, and set a team as the default for new users. The guide includes steps for testing the setup by creating a new user and verifying their ability to create keys within the designated team.
- [Anthropic File Usage](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/tutorials/anthropic_file_usage.md): A summary for anthropic_file_usage.md.
- [Using LiteLLM with OpenAI Codex](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/tutorials/openai_codex.md): This guide explains how to integrate OpenAI Codex with LiteLLM, enabling access to over 100 LLMs, including Gemini, through a familiar interface. It covers installation, proxy setup, model routing configuration, and how to run Codex with different models like Gemini and Claude. The guide also provides troubleshooting tips and links to additional resources.
- [Model Fallbacks with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/tutorials/model_fallbacks.md): This document explains how to implement model fallbacks using the LiteLLM library, covering basic fallback code and handling context window exceptions across multiple LLM providers like OpenAI, Anthropic, and Azure.
- [Team-based Routing](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/team_based_routing.md): This document explains how to set up team-based routing with LiteLLM, allowing calls to be routed to different model groups based on a team ID. It provides a configuration example with two model groups and instructions for setting up a PostgreSQL database, starting the proxy, creating teams with model aliases, generating team keys, and calling models using aliases.
- [Rules for LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/rules.md): This document outlines rules for LiteLLM, including pre_call_rules and post_call_rules, which allow for custom functions to control API calls based on input or output. It provides examples of how to implement these rules, such as failing calls with excessively long input or using them to trigger fallbacks when a model refuses to answer.
- [Projects built on LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/project.md): This document lists various projects built on LiteLLM, including AI assistants, coding tools, databases, and LLM application development frameworks, showcasing the versatility and extensibility of the LiteLLM library.
- [Data Retention Policy](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/data_retention.md): This policy outlines the requirements and controls for managing customer data retention and deletion within LiteLLM Cloud, covering active accounts, voluntary and involuntary closures, and data protection measures.
- [Apply Guardrail API for LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/apply_guardrail.md): An API endpoint to apply configured guardrails on LiteLLM instances, allowing direct calls for services needing guardrail integration. It processes input text based on specified guardrail names, languages, and entities, returning the modified text.
- [Moderation API](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/moderation.md): This document outlines the LiteLLM API for moderation, detailing its usage with Python SDK and Proxy Server. It specifies required and optional input parameters, including support for various input types like strings, arrays of strings, and multi-modal objects. The document also explains the output format, which adheres to OpenAI's standards, and lists supported providers.
- [Fine Tuning](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/fine_tuning.md): A summary for fine_tuning.md.
- [Realtime LiteLLM Proxy Usage](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/realtime.md): This document explains how to use the LiteLLM proxy for load balancing between Azure and OpenAI, covering configuration, starting the proxy, testing the connection, and logging options.
- [LiteLLM Batches API](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/batches.md): This document explains how to use the Batches API with LiteLLM. It covers creating files for batch completion, creating batch requests, listing batches, and retrieving specific batch and file content. It also details supported providers like OpenAI, Azure, and Vertex AI, and explains LiteLLM's cost tracking for batch processing.
- [Provider Files Endpoints](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/files_endpoints.md): This document outlines the file management capabilities within the liteLLM framework, detailing how to interact with provider's /files endpoints. It covers uploading, listing, retrieving, deleting files, and accessing their content, with code examples for both the OpenAI client and the liteLLM SDK. The document also lists supported providers including OpenAI, Azure OpenAI, and Vertex AI, along with a link to the Swagger API reference.
- [LiteLLM Assistants Documentation](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/assistants.md): This document provides a comprehensive guide to using LiteLLM for managing assistants, threads, and messages. It covers supported providers like OpenAI and Azure OpenAI, outlines quick start steps for creating and running assistants, and includes detailed code examples for SDK and cURL implementations. The documentation also explains how to handle streaming responses and configure the proxy API.
- [Rerank with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/rerank.md): This document provides guidance on using LiteLLM for reranking, detailing both Python SDK usage (with quick start and async examples) and LiteLLM Proxy integration. It also lists supported providers for reranking functionality.
- [Audio Transcription with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/audio_transcription.md): This document provides a guide on how to perform audio transcription using the LiteLLM Python SDK and LiteLLM Proxy. It covers quick start examples, configuration for OpenAI and Azure, and testing methods using cURL and the OpenAI Python SDK. It also lists supported providers for audio transcription, including Fireworks AI, Groq, and Deepgram.
- [Image Generation with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/image_generation.md): This document provides a comprehensive guide to using LiteLLM for image generation. It covers quick start examples using the LiteLLM Python SDK and Proxy, detailing essential input parameters and their respective models like DALL-E 2, DALL-E 3, and Stable Diffusion. The guide also includes Azure OpenAI and VertexAI integration examples, along with notes on OpenAI-compatible endpoints.
- [Model Context Protocol (MCP) with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/mcp.md): This document outlines the Model Context Protocol (MCP) Gateway provided by LiteLLM, enabling a unified endpoint for MCP tools and access control via keys and teams. It details the MCP architecture, supported transports (Streamable HTTP and SSE), and configuration options through the LiteLLM UI or config.yaml. The guide also provides instructions for using MCP with various clients, including OpenAI API, LiteLLM Proxy, Cursor IDE, and a Python FastMCP client, with examples for both server-side and client-side credentials.
- [Add Rerank Provider to LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/adding_provider/new_rerank_provider.md): This document provides a step-by-step guide on how to add a new rerank provider to LiteLLM. It explains how to set up the transformation file, register the provider, handle it in the main API file, and add tests. LiteLLM follows the Cohere Rerank API format for all rerank providers.
- [Unified API for Anthropic Messages with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/anthropic_unified.md): This document explains how to use the LiteLLM SDK and Proxy Server to interact with Anthropic's /v1/messages API, supporting various LLM providers like OpenAI, Google Gemini, Vertex AI, and AWS Bedrock. It covers features such as cost tracking, logging, streaming, fallbacks, and load balancing, providing Python SDK examples for both streaming and non-streaming calls, as well as setup instructions for the LiteLLM Proxy Server.
- [text_completion](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/text_completion.md): Provides instructions and examples for using the LiteLLM SDK and proxy server for text completion tasks. Covers setup, input parameters (required and optional), output formats, and supported providers like OpenAI and Azure OpenAI.
- [LiteLLM Responses API Documentation](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/response_api.md): This document outlines the Responses API provided by LiteLLM, which is a beta feature compatible with OpenAI's API specification. It details supported features like cost tracking, logging, streaming, fallbacks, and load balancing across various LLM providers including OpenAI, Anthropic, Vertex AI, AWS Bedrock, and Google AI Studio. The document includes usage examples for the LiteLLM Python SDK and LiteLLM Proxy with OpenAI SDK for both streaming and non-streaming responses.
- [LiteLLM Managed Files with Finetuning](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/managed_finetuning.md): This document details how to use LiteLLM's managed files feature for finetuning models across various providers like OpenAI, Azure, and Vertex AI. It covers both Proxy Admin and Developer usage, including setup, key generation, file uploads, job creation, and management operations like retrieve, list, and cancel. The feature simplifies the finetuning process by allowing centralized control and access management through LiteLLM's proxy.
- [LiteLLM Managed Files with Batches](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/managed_batches.md): This document explains how to use LiteLLM's Managed Files feature with batches, covering both Proxy Admin and Developer Usage. It details setup, key generation, request creation, file uploads, batch creation and retrieval, content retrieval, and listing batches, with a comprehensive E2E example. The feature allows load balancing across Azure Batch deployments and controlling access by key, user, or team. It is available via the `litellm[proxy]` package or any `litellm` docker image.
- [LiteLLM Managed Files](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/litellm_managed_files.md): This document outlines how to use LiteLLM Managed Files, a feature enabling the reuse of the same file across different AI providers and controlling file access. It details the setup process, including configuring `config.yaml` and starting the LiteLLM proxy. The document also provides examples of storing files and using them with various models, as well as setting up file permissions for different users.
- [text_to_speech](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/text_to_speech.md): This document provides a comprehensive guide to using LiteLLM for text-to-speech (TTS) functionalities. It details both Python SDK usage with quick start and asynchronous examples, and LiteLLM proxy usage with cURL examples. The document also outlines supported TTS providers like OpenAI, Azure OpenAI, and Vertex AI, and includes advanced configurations like setting max request file size for audio processing.
- [LiteLLM Image Edits](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/image_edits.md): This page details LiteLLM's image editing functionality, which maps to OpenAI's `/images/edits` API endpoint. It covers supported features like cost tracking, logging, and fallbacks, along with various usage examples using the LiteLLM Python SDK and LiteLLM Proxy. The page also outlines the supported image edit parameters and response formats.
- [Langfuse SDK with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/pass_through/langfuse.md): This document explains how to use the Langfuse SDK with LiteLLM for passing through Langfuse endpoints. It covers setup, example usage with and without virtual keys, and how to log traces to Langfuse.
- [Assembly AI with LiteLLM Proxy](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/pass_through/assembly_ai.md): This document guides users on integrating LiteLLM with AssemblyAI endpoints, covering setup, authentication, and providing code examples for transcription tasks using both default and EU servers. It also mentions LiteLLM's support for all AssemblyAI features like cost tracking and logging.
- [Anthropic SDK with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/pass_through/anthropic_completion.md): This document outlines how to use the LiteLLM proxy to interact with Anthropic's API. It covers setup, example usage with cURL and Python SDK, and highlights key differences from direct API calls, including support for all Anthropic endpoints and streaming. The guide also explains how to use virtual keys for secure access.
- [OpenAI Passthrough](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/pass_through/openai_passthrough.md): This document explains the OpenAI Passthrough feature in LiteLLM, which allows users to access less common or newer OpenAI endpoints not yet fully supported by LiteLLM. It details when to use the passthrough instead of the native integration and provides usage examples for the Assistants API, including creating clients, assistants, threads, messages, and managing runs.
- [Mistral LiteLLM Integration Guide](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/pass_through/mistral.md): This document outlines how to use Mistral AI models through the LiteLLM proxy. It covers direct API calls, streaming support, and advanced usage with virtual keys, including setup and example cURL commands for various endpoints like OCR and chat completions.
- [VLLM Integration with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/pass_through/vllm.md): This document outlines how to integrate VLLM with LiteLLM, enabling pass-through endpoints for VLLM's native API format. It details supported features like logging and streaming, provides configuration examples for models and API calls, and explains advanced usage with virtual keys for secure access. The integration allows users to leverage VLLM endpoints through the LiteLLM proxy with simplified authentication.
- [Cohere SDK with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/pass_through/cohere.md): This document provides a guide on using the Cohere SDK with LiteLLM, detailing supported features such as cost tracking, logging, and streaming. It includes examples of how to replace Cohere API endpoints with LiteLLM proxy endpoints for various functionalities like chat, reranking, and embeddings, along with instructions for setting up and testing the integration.
- [Pass Through Endpoints](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/pass_through.md): This document explains how to create pass-through endpoints using the LiteLLM proxy, enabling requests to be routed to external APIs like Bria and Mistral OCR. It details configuration steps through both a UI and a config.yaml file, including setting up routes, authentication, and pricing. The guide also covers testing endpoints and advanced features like enterprise authentication and custom adapters.
- [LiteLLM Input Parameters](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/completion/input.md): This document outlines the common and optional input parameters for the LiteLLM library, which acts as a translator for OpenAI Chat Completion parameters across various providers. It details required fields like model and messages, along with optional parameters such as temperature, top_p, stream, and stop sequences. The document also provides usage examples and a table summarizing parameter support across different LLM providers.
- [Azure OpenAI with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers/azure.md): This document provides a comprehensive guide on integrating with Azure OpenAI Service using the LiteLLM SDK and proxy. It details API key setup, usage examples for chat completions, and lists supported Azure OpenAI models including vision capabilities. The guide also demonstrates how to use LiteLLM with Langchain and cURL requests.
- [Wildcard Routing](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/wildcard_routing.md): A summary for wildcard_routing.md.
- [Budget Routing in LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/provider_budget_routing.md): This document explains how to implement budget routing in LiteLLM using provider, model, and tag-based budgets. It details the configuration process via `proxy_config.yaml`, provides request examples, and explains the underlying logic involving Redis for tracking and enforcing budget limits across different LLM providers.
- [Tag Based Routing with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/tag_routing.md): This document outlines how to implement tag-based routing in LiteLLM for various use cases, including managing user tiers and controlling model access for different teams. It details configuration steps in `config.yaml`, making requests with specific tags (e.g., "free", "paid"), and using request headers for routing. The document also covers setting default tags for untagged requests and introduces enterprise features like team-based tag routing for advanced access control.
- [Timeouts in LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/timeout.md): This document details how to configure and manage timeouts within the LiteLLM library. It covers global timeouts, per-model custom timeouts (including stream timeouts), dynamic per-request timeouts, and how to test timeout handling with mock values. Examples are provided for both SDK and proxy usage.
- [Load Balancing in LiteLLM Proxy](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/load_balancing.md): This document provides a guide on how to implement load balancing using LiteLLM proxy. It covers setting up multiple model instances, configuring routing strategies, and testing the load balancing functionality. The guide also explains how to use Redis for shared state across multiple LiteLLM proxy instances in environments like Kubernetes.
- [Request Prioritization with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/scheduler.md): This document explains how to use LiteLLMs request prioritization feature to manage high-traffic LLM API requests. It covers adding requests to a priority queue, polling the queue for availability, and setting priorities. The document provides code examples for both the LiteLLM SDK and LiteLLM Proxy, including integration with Redis for distributed prioritization.
- [Guardrails - Quick Start with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/guardrails/quick_start.md): This document provides a quick start guide for implementing guardrails with LiteLLM, covering prompt injection detection and PII masking. It details how to configure guardrails in `config.yaml`, including supported modes like pre_call and post_call, and how to start the LiteLLM Gateway. The guide also includes examples of test requests, expected responses for both successful and PII-violating calls, and explanations for using the 'default_on' feature and applying guardrails client-side.
- [Wandb Integration](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/observability/wandb_integration.md): This document explains how to integrate Weights & Biases with liteLLM to log LLM input/output. It covers prerequisites like installing the wandb library and provides a quick start guide with a two-line code snippet to enable automatic logging across all providers. The document also includes community support channels like Discord and contact information for the founders.
- [Supabase Integration for LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/observability/supabase_integration.md): This tutorial explains how to integrate Supabase with LiteLLM to log requests and track spend across various LLM providers. It covers creating a Supabase table, using success and failure callbacks, identifying end-users, and customizing table names. It also provides support channels for users.
- [Slack Integration for LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/observability/slack_integration.md): This document provides a guide on integrating LiteLLM with Slack for logging LLM inputs, outputs, and exceptions. It covers prerequisites like installing the LiteLLM library and obtaining a Slack webhook URL. The guide details how to create a custom callback function to send alerts to Slack and how to pass this callback to LiteLLM for both successful and failed completions. Instructions and code examples are provided for quick setup.
- [Sentry - Log LLM Exceptions with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/observability/sentry.md): This document explains how to integrate LiteLLM with Sentry for error monitoring. It covers setting up the SENTRY_DSN, using callbacks for completion and async completion calls, and configuring sample rate options for error and transaction logging. It also provides instructions on redacting sensitive information from Sentry logs.
- [Scrub Logged Data with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/observability/scrub_data.md): This document explains how to redact or mask personally identifiable information (PII) in logged data before sending it to logging integrations like Langfuse. It provides a step-by-step guide on setting up a custom callback using LiteLLM's CustomLogger class, demonstrating how to modify request and response data to mask sensitive information. The guide also shows how to connect this custom handler to LiteLLM and includes example code for testing the setup with both synchronous and asynchronous completion requests.
- [Raw Request/Response Logging with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/observability/raw_request_response.md): This document details how to enable raw request and response logging within the LiteLLM SDK and Proxy. It covers setting up integrations with providers like Langfuse, configuring environment variables, and includes code examples for both SDK and Proxy usage. The document also explains how to return raw response headers, specifically for OpenAI, with setup instructions and cURL examples.
- [Promptlayer Integration with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/observability/promptlayer_integration.md): This document explains how to integrate Promptlayer with LiteLLM to log requests across various LLM providers. It details the use of callbacks and logging metadata with code examples for OpenAI and Cohere.
- [Arize Phoenix OSS Integration with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/observability/phoenix_integration.md): This document provides a guide on integrating Arize Phoenix OSS, an open-source tracing and evaluation platform, with LiteLLM. It covers prerequisites, a quick start installation, and configuration for both direct code integration and use with the LiteLLM Proxy. The guide also includes links to relevant resources and support channels.
- [Comet Opik - Logging + Evals with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/observability/opik_integration.md): This document provides a guide on integrating Comet Opik, an open-source LLM evaluation platform, with LiteLLM. It covers setting up Opik, logging LLM prompts and responses, running evaluations for accuracy and other metrics, and using Opik-specific parameters like project name and tags within code examples for both SDK and proxy usage. Contact information and support channels are also provided.
- [OpenTelemetry Integration for LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/observability/opentelemetry_integration.md): Integrate OpenTelemetry for tracing LLM calls with various observability tools like Jaeger, Datadog, and Traceloop. This guide covers installation, environment variable setup for different backends (Traceloop, OTel Collector, Laminar), and a simple code snippet to enable logging. It also details how to redact sensitive information (messages, responses) from logs using environment variables or specific call parameters. Support and troubleshooting tips, including tracing user and key information on failed requests, are also provided.
- [MLflow for LLM Observability](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/observability/mlflow.md): MLflow is an open-source MLOps platform that supports experiment tracking, model management, evaluation, and observability (tracing) for LLM applications. It integrates with LiteLLM for enhanced observability compatible with OpenTelemetry. The platform allows users to install MLflow, enable auto-tracing for LiteLLM, and view logged traces in the MLflow UI. It also supports tracing tool calls, qualitative assessment of LLMs, and exporting traces to OpenTelemetry collectors. Users can combine MLflow traces with custom application traces for a unified view of LLM application performance.
- [Lunary Integration with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/observability/lunary_integration.md): This document provides a guide on integrating Lunary, an open-source platform for GenAI observability, with the LiteLLM SDK and proxy server. It covers setup, usage examples with Python SDK, LangChain, prompt templates, custom chains, and the LiteLLM proxy server. The guide also includes support contact information.
- [Logfire Integration with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/observability/logfire_integration.md): This document outlines the integration of Logfire, an open-source observability platform, with LiteLLM. It provides instructions on setting up the integration, including necessary package installations and environment variable configurations. The guide also details how to use Logfire’s features for detailed production traces, quality, cost, and latency analysis of LLM applications, and offers support channels for users.
- [LiteralAI Integration for LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/observability/literalai_integration.md): This document provides a guide on integrating LiteralAI with LiteLLM for logging, evaluation, and analytics of LLM applications. It covers installation, quick start examples, multi-step trace capabilities using SDK decorators, binding generations to prompt templates, and OpenAI proxy usage with LiteralAI instrumentation.
- [Langtrace Integration with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/observability/langtrace_integration.md): This document explains how to integrate Langtrace AI with LiteLLM to monitor, evaluate, and improve LLM applications. It provides prerequisites, quick start code examples for direct integration and usage with LiteLLM Proxy, and emphasizes the ease of logging responses across all providers with just two lines of code.
- [Langsmith Integration with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/observability/langsmith_integration.md): This document provides a guide on integrating LiteLLM with Langsmith for logging LLM inputs and outputs. It covers prerequisites, quick start implementation using a simple SDK or LiteLLM Proxy setup, and advanced configurations such as controlling batch size for local testing, setting specific Langsmith fields like run name and tags, and customizing the Langsmith base URL for local instances. The guide also includes links to support and community channels.
- [Langfuse OpenTelemetry Integration](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/observability/langfuse_otel_integration.md): This document outlines how to integrate Langfuse with LiteLLM using the OpenTelemetry protocol for enhanced observability. It covers features such as automatic trace collection, support for various Langfuse instances, and configuration via environment variables. The guide also details usage for basic and advanced setups, manual OpenTelemetry configuration, and integration with LiteLLM Proxy. It explains the data collected, authentication methods, and provides troubleshooting tips.
- [Langfuse Integration with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/observability/langfuse_integration.md): This document outlines how to integrate LiteLLM with Langfuse, an open-source LLM platform. It details the setup process for both the LiteLLM Proxy and the Python SDK, including code examples for logging LLM input/output, managing prompts, and evaluating applications. The guide covers advanced configurations like setting custom trace IDs, generation names, and metadata for enhanced observability.
- [Lago Usage Based Billing with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/observability/lago.md): This document provides a guide on integrating Lago, a usage-based billing solution, with LiteLLM. It details how to log costs and usage across various LLM providers using just a few lines of code. The guide includes quick start instructions for both SDK and Proxy usage, along with advanced insights into the data structure logged to Lago.
- [Humanloop Integration with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/observability/humanloop.md): This document explains how to integrate Humanloop with LiteLLM for managing AI features. It covers setup using SDK and Proxy, including configuration, testing with cURL and OpenAI SDK, and setting models on both LiteLLM and Humanloop platforms.
- [Prompt Injection Detection](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/guardrails/prompt_injection.md): This document outlines how to detect prompt injection attacks using liteLLM. It details two methods: similarity checking against a pre-generated list of attacks and using an LLM API for advanced checks. Configuration steps and example requests are provided for both methods.
- [PII, PHI Masking - Presidio | liteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/guardrails/pii_masking_v2.md): This document explains how to use the Presidio guardrail in LiteLLM for masking Personally Identifiable Information (PII) and Protected Health Information (PHI). It covers overview, deployment options, quick start guides for both LiteLLM UI and config.yaml, entity type configuration, and supported actions like MASK and BLOCK.
- [Pangea Guardrail for LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/guardrails/pangea.md): This document provides a guide to integrating Pangea's AI Guard service with LiteLLM for enhanced security. It covers configuration steps, including setting up the Pangea AI Guard service and modifying LiteLLM's config.yaml file. The guide also demonstrates how to start the LiteLLM Proxy and make requests, with examples of blocked and permitted responses, and how PII is redacted. Finally, it offers next steps for further customization and monitoring.
- [Lasso Security for LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/guardrails/lasso_security.md): This document provides a guide on integrating Lasso Security with LiteLLM to protect LLM applications from prompt injection attacks. It covers quick start instructions for defining guardrails, setting up the LiteLLM Gateway, and testing requests with examples of both successful and unsuccessful calls. It also details advanced configuration options such as user and conversation tracking for enhanced security monitoring.
- [Lakera AI Integration with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/guardrails/lakera_ai.md): This document provides a guide on integrating Lakera AI guardrails with LiteLLM. It covers defining guardrails in the LiteLLM config.yaml, starting the LiteLLM Gateway, and testing requests with examples of successful and unsuccessful calls, including expected error responses when content safety policies are violated.
- [Guardrails.ai Integration with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/guardrails/guardrails_ai.md): This document provides a guide on integrating Guardrails.ai with LiteLLM to add checks to LLM outputs. It covers setting up the Guardrails AI server, configuring LiteLLM with Guardrails.ai, and demonstrates usage with cURL examples. It also explains how to control Guardrails per project using API keys, a feature available in their Enterprise offering.
- [Custom Guardrail for LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/guardrails/custom_guardrail.md): This document provides a guide on creating and implementing custom guardrails in LiteLLM. It details the four available hook methods: async_pre_call_hook, async_moderation_hook, async_post_call_success_hook, and async_post_call_streaming_iterator_hook. The guide includes an example CustomGuardrail class, instructions on how to configure it in `config.yaml`, and how to run and test the LiteLLM Gateway with custom guardrails.
- [Bedrock Guardrails in LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/guardrails/bedrock.md): This document explains how to integrate Bedrock Guardrails with LiteLLM for enhanced content moderation. It covers setup through the LiteLLM config.yaml, including defining guardrails, choosing modes (pre_call, post_call, during_call), and provides examples of successful and               unsuccessful requests. The document also details how to enable PII masking for both requests and responses by configuring specific             parameters in the LiteLLM config.
- [Aporia LiteLLM Integration Guide](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/guardrails/aporia_api.md): This document provides a guide on integrating Aporia with LiteLLM to detect PII in requests and profanity in responses. It details the setup process, including creating Aporia projects, defining guardrails in the LiteLLM config.yaml, and testing requests. The guide also covers advanced features like controlling guardrails per API key.
- [Aim Security Integration with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/guardrails/aim_security.md): This document provides a step-by-step guide on integrating Aim Security with LiteLLM. It covers creating and configuring Aim Guards, adding the Aim Guardrail to the LiteLLM config.yaml file, and making requests to the LiteLLM Gateway. The guide also explains how to use Aim Guard policies for detecting and blocking sensitive information like PII and how to apply user-specific policies using email headers.
- [Helicone Integration with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/observability/helicone_integration.md): This document explains how to integrate Helicone, an open source LLM observability platform, with LiteLLM. It covers two integration methods: using callbacks and using Helicone as a proxy. The document also details advanced usage options such as custom metadata, caching, rate limiting, session tracking, and retry mechanisms.
- [Greenscale Integration with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/observability/greenscale_integration.md): Greenscale is a production monitoring platform for LLM-powered apps that tracks GenAI spending and responsible usage by capturing metadata. It integrates with LiteLLM using callbacks, requiring an API key and allowing additional metadata like project and application names to be sent for detailed usage tracking.
- [Evaluate LLMs with MLflow and AutoEval using LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/tutorials/eval_suites.md): This document provides a guide on evaluating Large Language Models (LLMs) using LiteLLM in integration with MLflow Evals and AutoEval. It covers the necessary prerequisites, setup steps, and code examples for both MLflow and AutoEval workflows. The guide demonstrates how to start the LiteLLM proxy, run MLflow evaluations with predefined metrics, and utilize AutoEval for assessing response factuality.
- [Google Cloud Storage Buckets Integration for LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/observability/gcs_bucket_integration.md): This document outlines how to integrate LiteLLM with Google Cloud Storage (GCS) buckets for logging LLM interactions. It details the configuration steps, including updating LiteLLM's config.yaml, setting environment variables for GCS bucket name and service account path, and starting the LiteLLM proxy. The document also explains how to retrieve the service account JSON file from the Google Cloud Console and provides expected log formats and support contact information. This feature is exclusive to LiteLLM Enterprise.
- [DeepEval Integration](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/observability/deepeval_integration.md): DeepEval is an open-source evaluation framework for LLMs, integrated with Confident AI for tracing and monitoring LLM applications. The Confident AI platform allows real-time issue detection, historical data analysis, human feedback collection, performance measurement, and cost/latency tracking. This document provides a quickstart guide for integrating DeepEval with liteLLM using API keys.
- [Braintrust Integration with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/observability/braintrust.md): Braintrust integrates with LiteLLM for AI product evaluation, logging, and data management, supporting OpenAI proxy usage and custom project configurations via metadata.
- [Athina Integration with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/observability/athina_integration.md): This document provides a guide on integrating Athina, an LLM evaluation and monitoring platform, with LiteLLM. It explains how to use LiteLLM callbacks to log requests across various LLM providers to the Athina dashboard. The guide also details how to include additional metadata with requests for enhanced tracking and analysis, and how to configure the integration for self-hosted Athina deployments.
- [Arize AI Integration with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/observability/arize_integration.md): This document outlines how to integrate Arize AI with LiteLLM for AI observability and evaluation. It covers initial setup, quick start guides for direct integration and LiteLLM Proxy, and instructions for passing Arize keys per request. The guide also provides contact information for support and community engagement.
- [Argilla with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/observability/argilla.md): Argilla is a collaborative data annotation tool that streamlines the process of building high-quality datasets for AI projects. This guide explains how to integrate Argilla with LiteLLM, covering setup, dataset creation using the Argilla SDK, and logging LLM interactions via the LiteLLM proxy, including how to configure data transformations and sampling rates for efficient data management.
- [AgentOps Integration with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/observability/agentops_integration.md): This document explains how to integrate AgentOps, an LLM observability platform, with LiteLLM to trace and monitor LLM calls. It covers basic integration using success callbacks, configuration options via environment variables, and advanced usage scenarios. The integration allows for detailed insights into AI operations across all providers with minimal code changes.
- [Audit Logs | liteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/multiple_admins.md): This document explains how to use LiteLLM's audit logs feature, which allows Proxy Admins to track the creation, update, deletion, and regeneration of entities like keys, teams, users, and models. It details how to enable audit logs, perform actions on entities, view logs in the LiteLLM UI, and specifies the API schema for audit log entries, including fields like id, updated_at, changed_by, action, and more.
- [Email Notifications](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/email.md): This document provides a comprehensive guide to setting up and utilizing email notifications within the LiteLLM Proxy. It covers supported events like user additions and API key creations, details integrations with Resend API and SMTP, and explains how to configure these settings. The guide also includes instructions for triggering emails, customizing templates, and branding email communications, with advanced branding options available for enterprise users.
- [Role-based Access Controls (RBAC) in LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/access_control.md): This document outlines the Role-based Access Control (RBAC) system in LiteLLM, detailing organizations, teams, internal users, roles, and virtual keys. It explains how to onboard organizations, create new organizations, add administrators, generate virtual keys, create teams, and add internal users to teams, specifically focusing on the 'org_admin' and 'internal_user' roles.
- [Service Accounts](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/service_accounts.md): A guide to using Service Accounts in LiteLLM for creating virtual keys for production projects, including setup, key creation, and testing with examples.
- [JWT-based Authentication in LiteLLM Proxy](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/token_auth.md): This document outlines how to use JSON Web Tokens (JWTs) for authentication with the LiteLLM proxy. It covers setting up JWT authentication, creating JWTs with specific scopes for users and projects, and testing the integration. The guide also details advanced configurations such as supporting multiple OpenID Connect providers, customizing scope names, tracking users and teams using JWT fields, and implementing custom JWT validation logic. Finally, it explains how to control model access based on teams and configure allowed routes for different JWT scopes.
- [Dynamic Callback Management in LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/dynamic_logging.md): LiteLLM's dynamic callback management allows teams to control logging per request via headers, essential for managing compliance and decentralized data handling in large-scale services. Users can list active callbacks and disable specific ones using the `x-litellm-disable-callbacks` header, supporting both single and multiple callback disabling with case-insensitive matching.
- [Team Logging with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/team_logging.md): This document provides a comprehensive guide on implementing team-based and key-based logging within LiteLLM. It details how to configure logging to different projects, disable logging for specific teams for compliance, and manage logging settings via API or configuration files. The guide also covers setting up callbacks for services like Langfuse and GCS buckets, and troubleshooting common issues.
- [StandardLoggingPayload Specification for liteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/logging_spec.md): This document details the StandardLoggingPayload, a data structure used for logging every successful and failed response in liteLLM. It outlines fields for identifying the call, tracking costs, monitoring status, managing tokens, and recording model and metadata information, including user API key details, request headers, and error specifics.
- [Customers / End-User Budgets](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/customers.md): This document outlines how to track customer spend and set budgets using LiteLLM. It details making API calls with customer IDs to track spending, retrieving customer spend information, and setting up budgets either through a quick start guide or by assigning pricing tiers. The guide also includes instructions for testing these features.
- [Internal User Self-Serve Guide for LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/self_serve.md): This document outlines how to enable self-serve capabilities for internal users within the LiteLLM proxy. It details the processes for allowing users to create their own API keys, view usage and caching analytics, and manage roles. The guide also covers advanced configurations such as setting up SSO for automatic team assignment, debugging JWT fields, customizing logout URLs, and managing user and team budgets. It provides step-by-step instructions for both UI and API interactions, along with relevant code snippets and expected responses.
- [Rate Limit Tiers](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/rate_limit_tiers.md): This document outlines how to define and manage rate limit tiers within LiteLLM Enterprise. It details the process of creating a budget with a specified rate limit (e.g., requests per minute) and assigning this budget to an API key to control access and usage. The document also provides code examples using curl to demonstrate creating budgets, generating keys associated with those budgets, and checking if the budget enforcement is active.
- [Temporary Budget Increase for LiteLLM Virtual Key](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/temporary_budget_increase.md): This document provides instructions on how to set a temporary budget increase for a LiteLLM Virtual Key. It details the process using cURL commands for key generation, updating with a temporary budget and expiry date, and testing the new budget. This feature is available for LiteLLM Enterprise users.
- [Vertex AI SDK](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/pass_through/vertex_ai.md): This page details how to use the Vertex AI SDK with LiteLLM, covering passthrough endpoints, supported features like cost tracking and streaming, and authentication methods. It provides examples for various use cases including Gemini API, Embeddings, Imagen, and more, with explanations for specific and default credential flows.
- [supported embeddings](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/embedding/supported_embedding.md): This document provides a comprehensive guide to using the LiteLLM library for embeddings, covering quick start examples, proxy usage, and detailed input parameter explanations. It also includes specific instructions for various embedding models such as OpenAI, Vertex AI, Bedrock, and Cohere, with examples for both SDK and proxy methods.
- [Providers](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/providers.md): liteLLM supports a wide range of providers, including OpenAI, Azure AI, Anthropic, AWS SageMaker, Vertex AI, Google AI Studio, Meta Llama, Mistral AI, Cohere, Anyscale, HuggingFace, Databricks, Deepgram, IBM watsonx.ai, Predibase, Nvidia NIM, Nscale, xAI, LM Studio, Cerebras, Volcano Engine, Triton Inference Server, Ollama, Perplexity AI, FriendliAI, Galadriel, Topaz, Groq, Github, Deepseek, Fireworks AI, Clarifai, VLLM, Llamafile, Infinity, Xinference, Cloudflare Workers AI, DeepInfra, AI21, NLP Cloud, Replicate, Together AI, Novita AI, Voyage AI, Jina AI, Aleph Alpha, Baseten, OpenRouter, SambaNova, Custom API Server, Petals, Snowflake, Featherless AI, and Nebius AI Studio.
- [Caching with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/caching.md): LiteLLM offers a caching system to store and reuse LLM responses, reducing costs and latency. It supports various caches including in-memory, disk, Redis, and Qdrant semantic caches. Configuration is done via `config.yaml`, with options for Redis namespaces, cluster, sentinel, and S3 bucket integration.
- [call_hooks](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/call_hooks.md): This document explains how to use call hooks in LiteLLM to modify or reject incoming requests before they are sent to the LLM API. It covers pre-call hooks for data modification, moderation hooks for content safety, and enforcing specific parameters like the 'user' field. The guide provides code examples and configuration details for integrating custom handlers into the LiteLLM proxy.
- [Secret Manager](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/secret.md): This document provides a comprehensive guide to integrating LiteLLM with various secret management systems, including Azure Key Vault, Google Secret Manager, Hashicorp Vault, and AWS Secret Manager. It details how to configure LiteLLM to securely read and write secrets such as API keys and virtual keys using these platforms. The guide covers authentication methods, configuration steps, and usage examples for each supported secret manager.
- [LiteLLM Spend Tracking](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/cost_tracking.md): LiteLLM enables automatic tracking of costs across over 100 LLMs, allowing users to monitor expenses by keys, users, and teams. It provides a detailed breakdown of spend through API requests, response headers, and a dedicated UI, with options for advanced usage like resetting spend data and generating custom reports for enterprise clients.
- [Control Model Access (liteLLM)](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/model_access.md): This document explains how to control model access in liteLLM through various methods including virtual keys, team IDs, and model access groups. It details how to restrict models to specific keys or teams, and how to use access groups for broader or segmented model access. The guide also covers wildcard model access for enterprise users, providing code examples for each scenario.
- [LiteLLM Proxy Examples](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/user_keys.md): This document provides examples of how to use the LiteLLM proxy with various tools and SDKs, including Langchain, OpenAI SDK, LlamaIndex, Instructor, and Curl. It details the compatibility with OpenAI, Azure OpenAI, Anthropic, and Vertex AI, and explains how to pass provider-specific parameters and metadata.
- [management_cli](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/management_cli.md): The litellm-proxy CLI is a command-line tool for managing your LiteLLM proxy server, with features for models, credentials, API keys, users, chat completions, and HTTP requests. It also includes a quick start guide and troubleshooting tips.
- [Enterprise Features for LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/enterprise.md): This document outlines the enterprise features of LiteLLM, a service that enhances LLM operations. It details security enhancements like SSO, audit logs, JWT authentication, and secret manager integration. The features also cover customized logging, logging guardrails, caching per project, spend tracking with budget controls, and export options for LLM logs. Additionally, it includes information on Prometheus metrics, custom branding, API key control for guardrails, and methods to block web crawlers and enforce required parameters for LLM requests.
- [architecture](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/architecture.md): This document outlines the high-level architecture and request flow of the LiteLLM proxy server. It details the steps from user request initiation, through virtual key verification, rate limiting, request routing, and finally to post-request processing such as logging and usage tracking.
- [Docker Deployment for LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/deploy.md): This document provides comprehensive instructions for deploying LiteLLM, a powerful tool for managing language model interactions. It covers Docker setup, including Dockerfile creation, quick start commands, and detailed `docker run` examples with configuration. The guide also explains how to use LiteLLM as a base image, build from its pip package, and provides configurations for Terraform and Kubernetes deployments. Additionally, it touches upon using the LiteLLM Helm chart for more advanced deployments.
- [configs](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/configs.md): This document details the configuration of the liteLLM proxy, explaining how to set up model lists, API bases, keys, and other parameters in the config.yaml file. It covers essential configurations like 'model_list' for defining model aliases and parameters, 'router_settings' for routing strategies, and 'litellm_settings' for module-level configurations. The guide also includes steps for starting the proxy with a custom configuration and testing the setup, along with examples for specific model integrations and embedding models.
- [Batching Completion with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/completion/batching.md): This document explains how to use LiteLLM for batch completions, including sending multiple completion calls to a single model, sending a single call to multiple models to get the fastest response, and sending a single call to multiple models to get all responses. It provides example code and output for each scenario.
- [Model Alias](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/completion/model_alias.md): This document explains how to use model aliases in LiteLLM, allowing users to map custom alias names to actual model names for easier API calls. It provides expected format and usage examples with relevant code snippets.
- [Function Calling with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/completion/function_call.md): This document explains how to use function calling with the LiteLLM library, including how to check for model support, perform parallel function calls, and provides a detailed code example for parallel function calling with gpt-3.5-turbo-1106.
- [UI Logs](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/ui_logs.md): This document explains how to view spend, token usage, API keys, and team names for each request to LiteLLM. It details how to enable logging for success and error logs, and how to opt-in to store request/response content. It also covers disabling error and spend logs, and configuring automatic deletion of old spend logs to maintain database performance.
- [LLM Credentials](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/ui_credentials.md): This document explains how to add and manage LLM provider credentials using the UI. It covers adding new credentials, using them with models, and creating credentials from existing models, along with security notes and FAQs regarding credential storage.
- [Event Hook for SSO Login (Custom Handler) | liteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/custom_sso.md): This document explains how to use a custom SSO handler with LiteLLM. It details the process, including the event flow and how to implement your own handler to run custom code after a user logs in via SSO. The guide provides a code example for creating the handler and explains how to configure LiteLLM to use it.
- [SCIM with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/tutorials/scim_litellm.md): This document provides a guide on how to use SCIM with LiteLLM for streamlined user provisioning. It details the setup process, including obtaining SCIM credentials, connecting Identity Providers (IDPs) like Azure AD and Okta, and testing the connection. The guide covers assigning groups to your LiteLLM app, signing in via SSO, and verifying the auto-creation of teams in the LiteLLM UI.
- [Post-Call Rules](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/rules.md): This document outlines how to implement post-call rules in the LiteLLM proxy to control LLM API responses. It provides a step-by-step guide on creating a Python file with custom rules, configuring the proxy to use these rules, and testing the implementation. The guide includes an example of a rule that checks the length of the model's response and demonstrates the expected error output when a rule is violated.
- [OpenID Connect (OIDC) with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/oidc.md): LiteLLM supports OpenID Connect (OIDC) for authentication, allowing users to avoid storing sensitive credentials. The document details supported OIDC Identity Providers and Relying Parties, configuration formats, and provides examples for integrating with services like Amazon Bedrock and Azure OpenAI. It also includes guidance on IAM role configuration and Azure AD application setup for secure authentication.
- [Session Logs for LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/ui_logs_sessions.md): This document explains how to group related requests into sessions using LiteLLM. It details methods for both the /chat/completions and /responses endpoints, demonstrating how to maintain conversation context through session IDs or previous response IDs across various tools like OpenAI Python SDK, Langchain, Curl, and LiteLLM Python SDK.
- [Allow Teams to Add Models](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/team_model_add.md): This document explains how to allow teams to add their own models and keys for specific projects, enabling all OpenAI calls to use their respective OpenAI keys. It covers how to specify a Team ID in the /model/add endpoint, provides examples for testing, and includes debugging steps for 'model_name' not found errors. This feature is particularly useful for teams working with their own fine-tuned models and is an enterprise-specific offering.
- [SSO for Admin UI](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/admin_ui_sso.md): This document details how to configure Single Sign-On (SSO) for the LiteLLM Admin UI, covering setup for various providers like Okta, Google, and Microsoft. It includes steps for configuring OAuth clients, environment variables, and callback URLs. Additionally, it explains how to set upper bounds for API keys, restrict email subdomains, manage proxy admin roles, disable the default team, use fallback username/password login, restrict UI access, and implement custom branding with custom logos and color schemes.
- [Streaming and Async with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/completion/stream.md): This document provides a comprehensive guide to using streaming and asynchronous functionalities within the LiteLLM SDK and Proxy. It covers how to implement streaming responses, asynchronous calls, and handle potential errors like infinite loops with code examples for each feature.
- [Prompt Formatting](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/completion/prompt_formatting.md): This document explains prompt formatting in LiteLLM, detailing how it automatically translates prompts for different models and allows for custom prompt templates. It covers support for Huggingface Chat Templates, provides examples for popular models, and demonstrates how to format prompts manually. The document also lists various providers and their corresponding prompt formatting details.
- [Master Key Rotation Steps for LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/master_key_rotations.md): This document outlines the recommended procedure for rotating the master key in LiteLLM. It details steps such as backing up the database, regenerating the master key via API, updating environment variables, and testing the new configuration to ensure seamless operation.
- [Configuration Settings for LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/config_settings.md): This document outlines the configuration settings for LiteLLM, a library for simplifying LLM integrations. It details settings for environment variables, model lists, callback configurations (including success, failure, and service callbacks), networking options, caching mechanisms (Redis, Qdrant Semantic Cache, S3), and various general settings like completion models, API key management, database connections, and request limits.
- [Debugging in LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/debugging.md): This document explains how to use LiteLLM for debugging, covering two levels: basic "debug" for info logs and "detailed debug" for debug logs. It also details how to enable JSON logs for structured output and provides guidance on controlling log verbosity by disabling default FastAPI logs and setting the LITELLM_LOG to "ERROR". Finally, it addresses common errors, such as "No available deployments," and offers solutions like adjusting cooldown times or disabling them, though the latter is not recommended.
- [Health Checks in LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/health.md): This document outlines the health check endpoints available in LiteLLM, including /health, /health/readiness, and /health/liveliness. It details how to perform health checks for various model types (embedding, image generation, text completion, speech, rerank, batch, realtime) and configurations like wildcard routes and background checks. It also covers options for hiding sensitive details and overriding health check timeouts.
- [Model Management in LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/model_management.md): This document outlines how to manage models in LiteLLM, covering configuration via config.yaml, retrieving model information using the /model/info endpoint, and adding new models without restarting the proxy via the /model/new API. It details the expected structure for model parameters and provides examples for adding custom model information.
- [CLI Arguments for LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/cli.md): This document outlines the various command-line interface (CLI) arguments available for the LiteLLM utility. It details default values, usage examples, and environment variable settings for each argument, including host, port, number of workers, API base and version, model selection, testing, debugging, and configuration options.
- [Best Practices for Production with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/proxy/prod.md): This document outlines best practices for deploying LiteLLM in a production environment. It covers configuration, machine specifications, Kubernetes deployment, Redis usage, logging, database migrations, and security considerations, providing a comprehensive guide for optimizing LiteLLM performance and reliability.
- [Drop Unsupported Params with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/completion/drop_params.md): This document explains how to use LiteLLM to drop unsupported parameters for various LLM providers. It covers enabling the drop_params feature globally or per-request, specifying parameters to drop using `additional_drop_params`, and allowing specific parameters like 'tools' via `allowed_openai_params` in both the SDK and Proxy Usage.
- [Pre-fix Assistant Messages for LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/completion/prefix.md): This document explains how to use pre-fix assistant messages in LiteLLM. It covers supported providers like Deepseek, Mistral, and Anthropic, and provides code examples for both SDK and proxy usage. The guide also includes instructions on how to check model support for this feature.
- [Knowledgebase](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/completion/knowledgebase.md): A summary for knowledgebase.md.
- [Predicted Outputs](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/completion/predict_outputs.md): Use Predicted Outputs when most of the LLM output is known beforehand, like in text or code rewrites, to significantly reduce latency. This feature is supported by OpenAI models and available from LiteLLM version v1.51.4.
- [Prompt Caching](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/completion/prompt_caching.md): A summary for prompt_caching.md.
- [liteLLM Reasoning Content](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/reasoning_content.md): This document explains liteLLM's "reasoning_content" and "thinking_blocks" feature, which standardizes and provides deeper insights into model responses. It details supported providers like Deepseek and Anthropic, provides code examples for SDK and proxy usage, and illustrates how to implement tool calling with "thinking" blocks.
- [Structured Outputs (JSON Mode) liteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/completion/json_mode.md): This document provides a comprehensive guide to implementing structured outputs, specifically JSON mode, using the liteLLM library. It covers quick start examples for SDK and proxy, how to check model compatibility for response formats and JSON schema, and detailed steps for passing JSON schema with examples for both SDK and cURL. The guide also explains how to enable client-side validation for JSON schema with liteLLM.
- [Vision Models in LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/completion/vision.md): This document provides a guide on using vision models with LiteLLM. It covers quick start examples for the LiteLLM Python SDK and Proxy Server, including how to pass images and define vision models in config.yaml. It also explains how to check if a model supports vision and how to explicitly specify the image type using the format parameter.
- [document_understanding](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/completion/document_understanding.md): This document explains how to use PDF input with the liteLLM library, covering various models like Vertex AI, Bedrock, Anthropic, and OpenAI. It provides code examples for sending PDF files via URL or base64 encoding, and how to specify the document format. Additionally, it shows how to check if a model supports PDF input.
- [Using Web Search with LiteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/completion/web_search.md): This document details how to use the web search functionality in LiteLLM, supporting various providers like OpenAI, xAI, Vertex AI, Gemini, and Perplexity. It covers usage via SDK and proxy for both /chat/completions and /responses endpoints, including configuration options for search context size within code and the config.yaml file.
- [Using Audio Models with liteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/completion/audio.md): This document provides a comprehensive guide on how to utilize audio capabilities with liteLLM, covering both audio input and output. It includes detailed code examples for the liteLLM Python SDK and Proxy Server, demonstrating how to send and receive audio data. The guide also explains how to check model compatibility for audio and details the expected JSON response format for audio data.
- [Calling Finetuned Models](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/guides/finetuned_models.md): This document outlines how to call finetuned models using the LiteLLM library, covering both OpenAI and Vertex AI platforms. It provides specific function call examples for OpenAI models like GPT-4 and GPT-3.5, and detailed setup instructions including environment variables, SDK usage, and configuration for Vertex AI finetuned models.
- [Provider-specific Params for liteLLM](https://raw.githubusercontent.com/dyingc/mcp_docs/master/docs_output/litellm/docs/completion/provider_specific_params.md): This document explains how to use provider-specific parameters with the liteLLM library. It covers passing non-OpenAI parameters like "top_k" directly in the completion() call or through provider-specific configuration variables. The document also provides SDK usage examples for various providers including OpenAI, Azure OpenAI, Anthropic, Huggingface, TogetherAI, Ollama, and Replicate.
