# Providers | liteLLM

## [ğŸ—ƒï¸ OpenAI3 items](/docs/providers/openai)## [ğŸ“„ï¸ OpenAI (Text Completion)LiteLLM supports OpenAI text completion models](/docs/providers/text_completion_openai)## [ğŸ“„ï¸ OpenAI-Compatible EndpointsSelecting openai as the provider routes your request to an OpenAI-compatible endpoint using the upstream](/docs/providers/openai_compatible)## [ğŸ—ƒï¸ Azure OpenAI3 items](/docs/providers/azure/)## [ğŸ“„ï¸ Azure AI StudioLiteLLM supports all models on Azure AI Studio](/docs/providers/azure_ai)## [ğŸ“„ï¸ AI/ML APIGetting started with the AI/ML API is simple. Follow these steps to set up your integration:](/docs/providers/aiml)## [ğŸ—ƒï¸ Vertex AI2 items](/docs/providers/vertex)## [ğŸ—ƒï¸ Google AI Studio3 items](/docs/providers/gemini)## [ğŸ“„ï¸ AnthropicLiteLLM supports all anthropic models.](/docs/providers/anthropic)## [ğŸ“„ï¸ AWS SagemakerLiteLLM supports All Sagemaker Huggingface Jumpstart Models](/docs/providers/aws_sagemaker)## [ğŸ—ƒï¸ Bedrock3 items](/docs/providers/bedrock)## [ğŸ“„ï¸ LiteLLM Proxy (LLM Gateway)| Property | Details |](/docs/providers/litellm_proxy)## [ğŸ“„ï¸ Meta Llama| Property | Details |](/docs/providers/meta_llama)## [ğŸ“„ï¸ Mistral AI APIhttps://docs.mistral.ai/api/](/docs/providers/mistral)## [ğŸ“„ï¸ Codestral API [Mistral AI]Codestral is available in select code-completion plugins but can also be queried directly. See the documentation for more details.](/docs/providers/codestral)## [ğŸ“„ï¸ CohereAPI KEYS](/docs/providers/cohere)## [ğŸ“„ï¸ Anyscalehttps://app.endpoints.anyscale.com/](/docs/providers/anyscale)## [ğŸ—ƒï¸ HuggingFace2 items](/docs/providers/huggingface)## [ğŸ“„ï¸ DatabricksLiteLLM supports all models on Databricks](/docs/providers/databricks)## [ğŸ“„ï¸ DeepgramLiteLLM supports Deepgram's /listen endpoint.](/docs/providers/deepgram)## [ğŸ“„ï¸ IBM watsonx.aiLiteLLM supports all IBM watsonx.ai foundational models and embeddings.](/docs/providers/watsonx)## [ğŸ“„ï¸ PredibaseLiteLLM supports all models on Predibase](/docs/providers/predibase)## [ğŸ“„ï¸ Nvidia NIMhttps://docs.api.nvidia.com/nim/reference/](/docs/providers/nvidia_nim)## [ğŸ“„ï¸ Nscale (EU Sovereign)https://docs.nscale.com/docs/inference/chat](/docs/providers/nscale)## [ğŸ“„ï¸ xAIhttps://docs.x.ai/docs](/docs/providers/xai)## [ğŸ“„ï¸ LM Studiohttps://lmstudio.ai/docs/basics/server](/docs/providers/lm_studio)## [ğŸ“„ï¸ Cerebrashttps://inference-docs.cerebras.ai/api-reference/chat-completions](/docs/providers/cerebras)## [ğŸ“„ï¸ Volcano Engine (Volcengine)https://www.volcengine.com/docs/82379/1263482](/docs/providers/volcano)## [ğŸ“„ï¸ Triton Inference ServerLiteLLM supports Embedding Models on Triton Inference Servers](/docs/providers/triton-inference-server)## [ğŸ“„ï¸ OllamaLiteLLM supports all models from Ollama](/docs/providers/ollama)## [ğŸ“„ï¸ Perplexity AI (pplx-api)https://www.perplexity.ai](/docs/providers/perplexity)## [ğŸ“„ï¸ FriendliAIWe support ALL FriendliAI models, just set friendliai/ as a prefix when sending completion requests](/docs/providers/friendliai)## [ğŸ“„ï¸ Galadrielhttps://docs.galadriel.com/api-reference/chat-completion-API](/docs/providers/galadriel)## [ğŸ“„ï¸ Topaz| Property | Details |](/docs/providers/topaz)## [ğŸ“„ï¸ Groqhttps://groq.com/](/docs/providers/groq)## [ğŸ“„ï¸ ğŸ†• Githubhttps://github.com/marketplace/models](/docs/providers/github)## [ğŸ“„ï¸ Deepseekhttps://deepseek.com/](/docs/providers/deepseek)## [ğŸ“„ï¸ Fireworks AIWe support ALL Fireworks AI models, just set fireworks_ai/ as a prefix when sending completion requests](/docs/providers/fireworks_ai)## [ğŸ“„ï¸ ClarifaiAnthropic, OpenAI, Mistral, Llama and Gemini LLMs are Supported on Clarifai.](/docs/providers/clarifai)## [ğŸ“„ï¸ VLLMLiteLLM supports all models on VLLM.](/docs/providers/vllm)## [ğŸ“„ï¸ LlamafileLiteLLM supports all models on Llamafile.](/docs/providers/llamafile)## [ğŸ“„ï¸ Infinity| Property | Details |](/docs/providers/infinity)## [ğŸ“„ï¸ Xinference [Xorbits Inference]https://inference.readthedocs.io/en/latest/index.html](/docs/providers/xinference)## [ğŸ“„ï¸ Cloudflare Workers AIhttps://developers.cloudflare.com/workers-ai/models/text-generation/](/docs/providers/cloudflare_workers)## [ğŸ“„ï¸ DeepInfrahttps://deepinfra.com/](/docs/providers/deepinfra)## [ğŸ“„ï¸ AI21LiteLLM supports the following AI21 models:](/docs/providers/ai21)## [ğŸ“„ï¸ NLP CloudLiteLLM supports all LLMs on NLP Cloud.](/docs/providers/nlp_cloud)## [ğŸ“„ï¸ ReplicateLiteLLM supports all models on Replicate](/docs/providers/replicate)## [ğŸ“„ï¸ Together AILiteLLM supports all models on Together AI.](/docs/providers/togetherai)## [ğŸ“„ï¸ Novita AI| Property | Details |](/docs/providers/novita)## [ğŸ“„ï¸ Voyage AIhttps://docs.voyageai.com/embeddings/](/docs/providers/voyage)## [ğŸ“„ï¸ Jina AIhttps://jina.ai/embeddings/](/docs/providers/jina_ai)## [ğŸ“„ï¸ Aleph AlphaLiteLLM supports all models from Aleph Alpha.](/docs/providers/aleph_alpha)## [ğŸ“„ï¸ BasetenLiteLLM supports any Text-Gen-Interface models on Baseten.](/docs/providers/baseten)## [ğŸ“„ï¸ OpenRouterLiteLLM supports all the text / chat / vision models from OpenRouter](/docs/providers/openrouter)## [ğŸ“„ï¸ SambaNovahttps://cloud.sambanova.ai/](/docs/providers/sambanova)## [ğŸ“„ï¸ Custom API Server (Custom Format)Call your custom torch-serve / internal LLM APIs via LiteLLM](/docs/providers/custom_llm_server)## [ğŸ“„ï¸ PetalsPetals//github.com/bigscience-workshop/petals](/docs/providers/petals)## [ğŸ“„ï¸ Snowflake| Property | Details |](/docs/providers/snowflake)## [ğŸ“„ï¸ Featherless AIhttps://featherless.ai/](/docs/providers/featherless_ai)## [ğŸ“„ï¸ Nebius AI Studiohttps://docs.nebius.com/studio/inference/quickstart](/docs/providers/nebius)