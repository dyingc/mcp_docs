# AI/ML API | liteLLM

On this page

Getting started with the AI/ML API is simple. Follow these steps to set up your integration:

### 1\. Get Your API Keyâ€‹

To begin, you need an API key. You can obtain yours here:  
ðŸ”‘ [Get Your API Key](https://aimlapi.com/app/keys/?utm_source=aimlapi&utm_medium=github&utm_campaign=integration)

### 2\. Explore Available Modelsâ€‹

Looking for a different model? Browse the full list of supported models:  
ðŸ“š [Full List of Models](https://docs.aimlapi.com/api-overview/model-database/text-models?utm_source=aimlapi&utm_medium=github&utm_campaign=integration)

### 3\. Read the Documentationâ€‹

For detailed setup instructions and usage guidelines, check out the official documentation:  
ðŸ“– [AI/ML API Docs](https://docs.aimlapi.com/quickstart/setting-up?utm_source=aimlapi&utm_medium=github&utm_campaign=integration)

### 4\. Need Help?â€‹

If you have any questions, feel free to reach out. Weâ€™re happy to assist! ðŸš€ [Discord](https://discord.gg/hvaUsJpVJf)

## Usageâ€‹

You can choose from LLama, Qwen, Flux, and 200+ other open and closed-source models on aimlapi.com/models. For example:
    
    
    import litellm  
      
    response = litellm.completion(  
        model="openai/meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo", # The model name must include prefix "openai" + the model name from ai/ml api  
        api_key="", # your aiml api-key   
        api_base="https://api.aimlapi.com/v2",  
        messages=[  
            {  
                "role": "user",  
                "content": "Hey, how's it going?",  
            }  
        ],  
    )  
    

## Streamingâ€‹
    
    
    import litellm  
      
    response = litellm.completion(  
        model="openai/Qwen/Qwen2-72B-Instruct",  # The model name must include prefix "openai" + the model name from ai/ml api  
        api_key="",  # your aiml api-key   
        api_base="https://api.aimlapi.com/v2",  
        messages=[  
            {  
                "role": "user",  
                "content": "Hey, how's it going?",  
            }  
        ],  
        stream=True,  
    )  
    for chunk in response:  
        print(chunk)  
    

## Async Completionâ€‹
    
    
    import asyncio  
      
    import litellm  
      
      
    async def main():  
        response = await litellm.acompletion(  
            model="openai/anthropic/claude-3-5-haiku",  # The model name must include prefix "openai" + the model name from ai/ml api  
            api_key="",  # your aiml api-key  
            api_base="https://api.aimlapi.com/v2",  
            messages=[  
                {  
                    "role": "user",  
                    "content": "Hey, how's it going?",  
                }  
            ],  
        )  
        print(response)  
      
      
    if __name__ == "__main__":  
        asyncio.run(main())  
    

## Async Streamingâ€‹
    
    
    import asyncio  
    import traceback  
      
    import litellm  
      
      
    async def main():  
        try:  
            print("test acompletion + streaming")  
            response = await litellm.acompletion(  
                model="openai/nvidia/Llama-3.1-Nemotron-70B-Instruct-HF", # The model name must include prefix "openai" + the model name from ai/ml api  
                api_key="", # your aiml api-key  
                api_base="https://api.aimlapi.com/v2",  
                messages=[{"content": "Hey, how's it going?", "role": "user"}],  
                stream=True,  
            )  
            print(f"response: {response}")  
            async for chunk in response:  
                print(chunk)  
        except:  
            print(f"error occurred: {traceback.format_exc()}")  
            pass  
      
      
    if __name__ == "__main__":  
        asyncio.run(main())  
    

## Async Embeddingâ€‹
    
    
    import asyncio  
      
    import litellm  
      
      
    async def main():  
        response = await litellm.aembedding(  
            model="openai/text-embedding-3-small", # The model name must include prefix "openai" + the model name from ai/ml api  
            api_key="",  # your aiml api-key  
            api_base="https://api.aimlapi.com/v1", # ðŸ‘ˆ the URL has changed from v2 to v1  
            input="Your text string",  
        )  
        print(response)  
      
      
    if __name__ == "__main__":  
        asyncio.run(main())  
    

## Async Image Generationâ€‹
    
    
    import asyncio  
      
    import litellm  
      
      
    async def main():  
        response = await litellm.aimage_generation(  
            model="openai/dall-e-3",  # The model name must include prefix "openai" + the model name from ai/ml api  
            api_key="",  # your aiml api-key  
            api_base="https://api.aimlapi.com/v1", # ðŸ‘ˆ the URL has changed from v2 to v1  
            prompt="A cute baby sea otter",  
        )  
        print(response)  
      
      
    if __name__ == "__main__":  
        asyncio.run(main())  
    

  * 1\. Get Your API Key
  * 2\. Explore Available Models
  * 3\. Read the Documentation
  * 4\. Need Help?
  * Usage
  * Streaming
  * Async Completion
  * Async Streaming
  * Async Embedding
  * Async Image Generation